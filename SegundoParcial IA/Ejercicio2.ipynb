{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcxWgfDK65g5y+w3BOJnuO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zelechos/IA_Code/blob/master/SegundoParcial%20IA/Ejercicio2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxop9AHk7ssN",
        "outputId": "0e5cdf28-5b20-45ce-a581-f44f1807ad99"
      },
      "source": [
        "# montamos nuestro drive al proyecto\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LkuPvlJPHUg"
      },
      "source": [
        "# Traemos nuestro dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QrewYU7d-GDM",
        "outputId": "8386bf99-a58e-44d1-810e-79220f59319e"
      },
      "source": [
        "#hacemos una copia para trabajar con el dataset\n",
        "import shutil\n",
        "\n",
        "shutil.copy(\"/content/drive/MyDrive/revisiones.json\",\"/content/revisiones.json\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/revisiones.json'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IbdumnGPKLA"
      },
      "source": [
        "# Realizamos el Preprocesamiento de Textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX0cMHQd7plb",
        "outputId": "90b969f2-8db3-4ca3-f4be-ddfefe3a3104"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/revisiones.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "#accedemos a la evaluation\n",
        "print(\"evaluation : \", data[\"paper\"][0][\"review\"][0][\"evaluation\"])\n",
        "\n",
        "#accedemos a la text\n",
        "print(\"text : \", data[\"paper\"][0][\"review\"][0][\"text\"])\n",
        "\n",
        "#averiguando la longitud\n",
        "print(\"longitud : \", len(data[\"paper\"][0][\"review\"]))\n",
        "\n",
        "paper = data[\"paper\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluation :  1\n",
            "text :  - El artículo aborda un problema contingente y muy relevante, e incluye tanto un diagnóstico nacional de uso de buenas prácticas como una solución (buenas prácticas concretas). - El lenguaje es adecuado.  - El artículo se siente como la concatenación de tres artículos diferentes: (1) resultados de una encuesta, (2) buenas prácticas de seguridad, (3) incorporación de buenas prácticas. - El orden de las secciones sería mejor si refleja este orden (la versión revisada es #2, #1, #3). - El artículo no tiene validación de ningún tipo, ni siquiera por evaluación de expertos.\n",
            "longitud :  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDzt5coZH9A8",
        "outputId": "25a73b69-6669-4df2-d49c-9ec23111bba0"
      },
      "source": [
        "# Capturamos todas las reviews en una lista\n",
        "Reviews = []\n",
        "\n",
        "cont = 0\n",
        "for x in paper:\n",
        "  # print(x[cont][\"review\"])\n",
        "  Reviews.append(data[\"paper\"][cont][\"review\"])\n",
        "  cont += 1\n",
        "\n",
        "print(len(Reviews))\n",
        "print(Reviews[171][0][\"text\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "172\n",
            "El artículo describe básicamente los componentes de un aerodeslizador, y el sensor que podría ser utilizado para el control de navegación.  Es un estudio más bien teórico que no presenta evidencias de su construcción.  No queda claro si la propuesta ya fue publicada por el inserto en pie de página.  Se deben corregir varios errores de tipo gramatical; Ejemplos: Primer párrafo introducción ”con el término” y no “el término” C. Sistema de propulsión … a los cuales se acoplará Texto figura 2 debe incorporarse al pie de la figura Figura 3 sin título Figura 6 después de la 3? Penúltimo párrafo hoja 4 (“con estés….”) Enumerar ecuaciones.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0VLtW32KDBm"
      },
      "source": [
        "# Tenemos reviews vacias 51,61,105"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvEpf-KUJAsI",
        "outputId": "e116cf22-fc94-4faf-c03c-af4c370b9e18"
      },
      "source": [
        "# print(len(Reviews[0]))\n",
        "cont = 1 \n",
        "# Obtenemos todas las longitudes de nuestra Lista de Reviews\n",
        "Longitudes = []\n",
        "for i in Reviews:\n",
        "  # print(cont ,\"---->\",i)\n",
        "  Longitudes.append(len(i))\n",
        "  cont += 1\n",
        "\n",
        "print(Longitudes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 1, 1, 3, 1, 2, 4, 0, 1, 3, 1, 4, 2, 3, 1, 2, 1, 0, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 2, 2, 3, 1, 1, 2, 4, 2, 1, 1, 1, 3, 2, 2, 2, 2, 2, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 0, 3, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meQBeIkFK8pR",
        "outputId": "e9c443b1-3c82-4b27-c56b-40b3481828e8"
      },
      "source": [
        "Evalutions = []\n",
        "Texts = []\n",
        "\n",
        "Data = []\n",
        "for index in range(len(Longitudes)):\n",
        "  # print(index)\n",
        "  for i in range(Longitudes[index]):\n",
        "    Data.append([ Reviews[index][i][\"evaluation\"] , Reviews[index][i][\"text\"]])\n",
        "    # Evalutions.append(Reviews[index][i][\"evaluation\"])\n",
        "    # Texts.append(Reviews[index][i][\"text\"])\n",
        "\n",
        "print(Data[404])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', 'El artículo describe básicamente los componentes de un aerodeslizador, y el sensor que podría ser utilizado para el control de navegación.  Es un estudio más bien teórico que no presenta evidencias de su construcción.  No queda claro si la propuesta ya fue publicada por el inserto en pie de página.  Se deben corregir varios errores de tipo gramatical; Ejemplos: Primer párrafo introducción ”con el término” y no “el término” C. Sistema de propulsión … a los cuales se acoplará Texto figura 2 debe incorporarse al pie de la figura Figura 3 sin título Figura 6 después de la 3? Penúltimo párrafo hoja 4 (“con estés….”) Enumerar ecuaciones.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIOdVY5tPsiW"
      },
      "source": [
        "# Contamos con 405 Textos para entrenar nuestro modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMz8Sx0QPrxO",
        "outputId": "6aa6df97-807d-4841-d43e-e3c41eaeab8d"
      },
      "source": [
        "print(len(Data))\n",
        "\n",
        "# 5 criterios de valoración\n",
        "# muy malo  = -2\n",
        "# malo  = -1\n",
        "# regular  = 0\n",
        "# bueno  = 1\n",
        "# muy bueno  = 2\n",
        "\n",
        "cont = 1 \n",
        "# for i in Data:\n",
        "#     # print(cont,\"--> \",i[0])\n",
        "#     if i[0] == '-2':\n",
        "#       i[0] = \"Muy Malo\"\n",
        "#     elif i[0] == '-1':\n",
        "#       i[0] = \"Malo\"\n",
        "#     elif i[0] == '0':\n",
        "#       i[0] = \"Regular\"\n",
        "#     elif i[0] == '1':\n",
        "#       i[0] = \"Bueno\"\n",
        "#     elif i[0] == '2':\n",
        "#       i[0] = \"Muy Bueno\"\n",
        "\n",
        "#     cont += 1\n",
        "\n",
        "for i in Data:\n",
        "    # print(cont,\"--> \",i[0])\n",
        "    if i[0] == '-2':\n",
        "      i[0] = -2\n",
        "    elif i[0] == '-1':\n",
        "      i[0] = -1\n",
        "    elif i[0] == '0':\n",
        "      i[0] = 0\n",
        "    elif i[0] == '1':\n",
        "      i[0] = 1\n",
        "    elif i[0] == '2':\n",
        "      i[0] = 2\n",
        "\n",
        "    cont += 1\n",
        "\n",
        "# for i in Data:\n",
        "#     # print(\"Valoracion : \", i[0])\n",
        "#     # print(\"Texto : \", i[1])\n",
        "#     print(i)\n",
        "\n",
        "# print(Data[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZu37hzp7n5q"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "Train = []\n",
        "Test = []\n",
        "v =[]\n",
        "t =[]\n",
        "for i in Data:\n",
        "    v.append(i[0])\n",
        "    t.append(i[1])\n",
        "\n",
        "\n",
        "data = {'Valoracion': v[:300],\n",
        "        'Texto': t[:300]}\n",
        "\n",
        "data1 = {'Valoracion': v[300:],\n",
        "        'Texto': t[300:]}\n",
        "\n",
        "os.mkdir('/content/Dataset')\n",
        "df = pd.DataFrame(data, columns = ['Valoracion', 'Texto'])\n",
        "df1 = pd.DataFrame(data1, columns = ['Valoracion', 'Texto'])\n",
        "df.to_csv('/content/Dataset/train.csv')\n",
        "df1.to_csv('/content/Dataset/test.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF78Uv5VPjKF",
        "outputId": "05e77624-9c7c-4acd-f431-bebe57fbdbaa"
      },
      "source": [
        "print(len(v[:300]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdb4i0jL7spw",
        "outputId": "82837b97-2e08-46c8-e72a-e36a168d8345"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuKQl-jb7snF"
      },
      "source": [
        "#python -m spacy download en\n",
        "spacy_en = spacy.load(\"en\")"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRiFWF937sfl"
      },
      "source": [
        "def tokenize(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtC0Tju83wBo"
      },
      "source": [
        "Texto = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n",
        "Valoracion = Field(sequential=False, use_vocab=False)\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulh7zIkt38tG"
      },
      "source": [
        "fields = {\"Texto\": (\"t\", Texto), \"Valoracion\": (\"v\", Valoracion)}"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnwxD-za5pi_"
      },
      "source": [
        "train_data, test_data = TabularDataset.splits(\n",
        "                                        path='/content/Dataset',\n",
        "                                        train='train.csv',\n",
        "                                        test='test.csv',\n",
        "                                        format='csv',\n",
        "                                        fields=fields)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwzALLayVSW6",
        "outputId": "141ccd32-0230-4680-e88d-7e956e8262f8"
      },
      "source": [
        "len(train_data) , len(test_data)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 105)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBtZ6YmVVagh",
        "outputId": "5af2a047-813f-4ba2-b44d-817e3803a004"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'t': ['-', 'el', 'artículo', 'aborda', 'un', 'problema', 'contingente', 'y', 'muy', 'relevante', ',', 'e', 'incluye', 'tanto', 'un', 'diagnóstico', 'nacional', 'de', 'uso', 'de', 'buenas', 'prácticas', 'como', 'una', 'solución', '(', 'buenas', 'prácticas', 'concretas', ')', '.', '-', 'el', 'lenguaje', 'es', 'adecuado', '.', ' ', '-', 'el', 'artículo', 'se', 'siente', 'como', 'la', 'concatenación', 'de', 'tres', 'artículos', 'diferentes', ':', '(', '1', ')', 'resultados', 'de', 'una', 'encuesta', ',', '(', '2', ')', 'buenas', 'prácticas', 'de', 'seguridad', ',', '(', '3', ')', 'incorporación', 'de', 'buenas', 'prácticas', '.', '-', 'el', 'orden', 'de', 'las', 'secciones', 'sería', 'mejor', 'si', 'refleja', 'este', 'orden', '(', 'la', 'versión', 'revisada', 'es', '#', '2', ',', '#', '1', ',', '#', '3', ')', '.', '-', 'el', 'artículo', 'no', 'tiene', 'validación', 'de', 'ningún', 'tipo', ',', 'ni', 'siquiera', 'por', 'evaluación', 'de', 'expertos', '.'], 'v': '1'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUbTP7q75peH"
      },
      "source": [
        "Texto.build_vocab(train_data, max_size=10000, min_freq=1,vectors=\"glove.6B.100d\")"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqdd7e9eh8jY",
        "outputId": "57565dbd-a6d4-4d16-b30f-caf13c74e3a0"
      },
      "source": [
        "Texto.vocab.freqs.most_common(15)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 3071),\n",
              " ('.', 2116),\n",
              " (',', 2043),\n",
              " ('la', 1580),\n",
              " ('el', 1446),\n",
              " ('en', 1349),\n",
              " ('que', 1022),\n",
              " (' ', 953),\n",
              " ('y', 943),\n",
              " ('se', 876),\n",
              " ('no', 708),\n",
              " ('un', 669),\n",
              " ('es', 630),\n",
              " ('a', 629),\n",
              " ('los', 586)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LIoJO48iDBP",
        "outputId": "03920a2c-7275-43b1-ece6-27c037b0f3bc"
      },
      "source": [
        "Texto.vocab.itos[:10]"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', 'de', '.', ',', 'la', 'el', 'en', 'que', ' ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjDmKD5AWL0G"
      },
      "source": [
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data), batch_size=2, device=device\n",
        ")"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6UKhlEB5pcJ"
      },
      "source": [
        "class RNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size, num_layers):\n",
        "        super(RNN_LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embed_size)\n",
        "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.fc_out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device)\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, _ = self.rnn(embedded, (h0, c0))\n",
        "        prediction = self.fc_out(outputs[-1, :, :])\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUppRqm65pZo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "2c35c0d7-c374-4b2f-837f-db7880d60e97"
      },
      "source": [
        "# Hyperparameters\n",
        "input_size = len(Texto.vocab)\n",
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "embedding_size = 100\n",
        "learning_rate = 0.005\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize network\n",
        "model = RNN_LSTM(input_size, embedding_size, hidden_size, num_layers).to(device)\n"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-f0322d4cede1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Initialize network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5bJUvKNReQh"
      },
      "source": [
        "# (NOT COVERED IN YOUTUBE VIDEO): Load the pretrained embeddings onto our model\n",
        "pretrained_embeddings = Texto.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juqGZBHyuYjW"
      },
      "source": [
        "# Entrenamos la Red Neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHd33sDwReOZ",
        "outputId": "8bf76f5e-b8c3-4760-d088-4ae81375f379"
      },
      "source": [
        "# Train Network\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        # Get data to cuda if possible\n",
        "        data = batch.t.to(device=device)\n",
        "        targets = batch.v.to(device=device)\n",
        "\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores.squeeze(1), targets.type_as(scores))\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent\n",
        "        optimizer.step()\n",
        "\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} loss {loss:.5f} val_loss {loss:.5f} \")\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 loss -1.05169 val_loss -1.05169 \n",
            "Epoch 1/10 loss 7.92041 val_loss 7.92041 \n",
            "Epoch 1/10 loss 0.13661 val_loss 0.13661 \n",
            "Epoch 1/10 loss -3.34928 val_loss -3.34928 \n",
            "Epoch 1/10 loss -1.57453 val_loss -1.57453 \n",
            "Epoch 1/10 loss 3.19471 val_loss 3.19471 \n",
            "Epoch 1/10 loss -0.55456 val_loss -0.55456 \n",
            "Epoch 1/10 loss 1.75764 val_loss 1.75764 \n",
            "Epoch 1/10 loss -3.63168 val_loss -3.63168 \n",
            "Epoch 1/10 loss 2.93102 val_loss 2.93102 \n",
            "Epoch 1/10 loss -1.20552 val_loss -1.20552 \n",
            "Epoch 1/10 loss -4.88862 val_loss -4.88862 \n",
            "Epoch 1/10 loss -2.13480 val_loss -2.13480 \n",
            "Epoch 1/10 loss 2.89991 val_loss 2.89991 \n",
            "Epoch 1/10 loss 1.17145 val_loss 1.17145 \n",
            "Epoch 1/10 loss 2.92602 val_loss 2.92602 \n",
            "Epoch 1/10 loss 0.85164 val_loss 0.85164 \n",
            "Epoch 1/10 loss -3.01220 val_loss -3.01220 \n",
            "Epoch 1/10 loss 1.07766 val_loss 1.07766 \n",
            "Epoch 1/10 loss -3.63266 val_loss -3.63266 \n",
            "Epoch 1/10 loss 1.51365 val_loss 1.51365 \n",
            "Epoch 1/10 loss 2.20880 val_loss 2.20880 \n",
            "Epoch 1/10 loss -5.04937 val_loss -5.04937 \n",
            "Epoch 1/10 loss 4.11528 val_loss 4.11528 \n",
            "Epoch 1/10 loss -1.79968 val_loss -1.79968 \n",
            "Epoch 1/10 loss -2.35821 val_loss -2.35821 \n",
            "Epoch 1/10 loss -1.00799 val_loss -1.00799 \n",
            "Epoch 1/10 loss 3.01489 val_loss 3.01489 \n",
            "Epoch 1/10 loss 4.33770 val_loss 4.33770 \n",
            "Epoch 1/10 loss -1.22971 val_loss -1.22971 \n",
            "Epoch 1/10 loss -2.57838 val_loss -2.57838 \n",
            "Epoch 1/10 loss -0.66062 val_loss -0.66062 \n",
            "Epoch 1/10 loss -1.70170 val_loss -1.70170 \n",
            "Epoch 1/10 loss -3.38765 val_loss -3.38765 \n",
            "Epoch 1/10 loss 2.28764 val_loss 2.28764 \n",
            "Epoch 1/10 loss 2.30226 val_loss 2.30226 \n",
            "Epoch 1/10 loss 1.80250 val_loss 1.80250 \n",
            "Epoch 1/10 loss 0.37762 val_loss 0.37762 \n",
            "Epoch 1/10 loss 0.98871 val_loss 0.98871 \n",
            "Epoch 1/10 loss -6.80461 val_loss -6.80461 \n",
            "Epoch 1/10 loss 1.06102 val_loss 1.06102 \n",
            "Epoch 1/10 loss 2.82563 val_loss 2.82563 \n",
            "Epoch 1/10 loss -3.44598 val_loss -3.44598 \n",
            "Epoch 1/10 loss -1.84781 val_loss -1.84781 \n",
            "Epoch 1/10 loss -7.11573 val_loss -7.11573 \n",
            "Epoch 1/10 loss -6.54944 val_loss -6.54944 \n",
            "Epoch 1/10 loss 1.59770 val_loss 1.59770 \n",
            "Epoch 1/10 loss -1.59635 val_loss -1.59635 \n",
            "Epoch 1/10 loss -3.34000 val_loss -3.34000 \n",
            "Epoch 1/10 loss 0.10695 val_loss 0.10695 \n",
            "Epoch 1/10 loss -6.10377 val_loss -6.10377 \n",
            "Epoch 1/10 loss 0.09756 val_loss 0.09756 \n",
            "Epoch 1/10 loss -1.17290 val_loss -1.17290 \n",
            "Epoch 1/10 loss -4.15853 val_loss -4.15853 \n",
            "Epoch 1/10 loss 5.06705 val_loss 5.06705 \n",
            "Epoch 1/10 loss 3.26046 val_loss 3.26046 \n",
            "Epoch 1/10 loss -8.75128 val_loss -8.75128 \n",
            "Epoch 1/10 loss 1.72173 val_loss 1.72173 \n",
            "Epoch 1/10 loss -3.51782 val_loss -3.51782 \n",
            "Epoch 1/10 loss 1.25417 val_loss 1.25417 \n",
            "Epoch 1/10 loss 0.01766 val_loss 0.01766 \n",
            "Epoch 1/10 loss -1.22988 val_loss -1.22988 \n",
            "Epoch 1/10 loss 0.30128 val_loss 0.30128 \n",
            "Epoch 1/10 loss -2.39329 val_loss -2.39329 \n",
            "Epoch 1/10 loss 0.01407 val_loss 0.01407 \n",
            "Epoch 1/10 loss -8.52791 val_loss -8.52791 \n",
            "Epoch 1/10 loss 5.98652 val_loss 5.98652 \n",
            "Epoch 1/10 loss 1.55595 val_loss 1.55595 \n",
            "Epoch 1/10 loss 8.56348 val_loss 8.56348 \n",
            "Epoch 1/10 loss 0.00530 val_loss 0.00530 \n",
            "Epoch 1/10 loss 3.98586 val_loss 3.98586 \n",
            "Epoch 1/10 loss 7.07556 val_loss 7.07556 \n",
            "Epoch 1/10 loss -10.02653 val_loss -10.02653 \n",
            "Epoch 1/10 loss -4.97554 val_loss -4.97554 \n",
            "Epoch 1/10 loss 2.42904 val_loss 2.42904 \n",
            "Epoch 1/10 loss -3.10851 val_loss -3.10851 \n",
            "Epoch 1/10 loss -5.72859 val_loss -5.72859 \n",
            "Epoch 1/10 loss 0.00640 val_loss 0.00640 \n",
            "Epoch 1/10 loss -2.24603 val_loss -2.24603 \n",
            "Epoch 1/10 loss -0.62399 val_loss -0.62399 \n",
            "Epoch 1/10 loss -6.55434 val_loss -6.55434 \n",
            "Epoch 1/10 loss 2.21111 val_loss 2.21111 \n",
            "Epoch 1/10 loss 5.41696 val_loss 5.41696 \n",
            "Epoch 1/10 loss 4.83754 val_loss 4.83754 \n",
            "Epoch 1/10 loss 4.93024 val_loss 4.93024 \n",
            "Epoch 1/10 loss 3.61692 val_loss 3.61692 \n",
            "Epoch 1/10 loss -5.32030 val_loss -5.32030 \n",
            "Epoch 1/10 loss 4.75986 val_loss 4.75986 \n",
            "Epoch 1/10 loss 4.12725 val_loss 4.12725 \n",
            "Epoch 1/10 loss 2.64909 val_loss 2.64909 \n",
            "Epoch 1/10 loss 8.55353 val_loss 8.55353 \n",
            "Epoch 1/10 loss 6.53281 val_loss 6.53281 \n",
            "Epoch 1/10 loss 5.21984 val_loss 5.21984 \n",
            "Epoch 1/10 loss -4.14713 val_loss -4.14713 \n",
            "Epoch 1/10 loss -3.98887 val_loss -3.98887 \n",
            "Epoch 1/10 loss -0.93605 val_loss -0.93605 \n",
            "Epoch 1/10 loss -1.39307 val_loss -1.39307 \n",
            "Epoch 1/10 loss 1.23944 val_loss 1.23944 \n",
            "Epoch 1/10 loss 3.59430 val_loss 3.59430 \n",
            "Epoch 1/10 loss 4.67463 val_loss 4.67463 \n",
            "Epoch 1/10 loss 2.25000 val_loss 2.25000 \n",
            "Epoch 1/10 loss 3.06110 val_loss 3.06110 \n",
            "Epoch 1/10 loss 5.72367 val_loss 5.72367 \n",
            "Epoch 1/10 loss 0.21643 val_loss 0.21643 \n",
            "Epoch 1/10 loss 1.43730 val_loss 1.43730 \n",
            "Epoch 1/10 loss 2.13843 val_loss 2.13843 \n",
            "Epoch 1/10 loss 0.13331 val_loss 0.13331 \n",
            "Epoch 1/10 loss 0.46016 val_loss 0.46016 \n",
            "Epoch 1/10 loss 0.70332 val_loss 0.70332 \n",
            "Epoch 1/10 loss 0.44355 val_loss 0.44355 \n",
            "Epoch 1/10 loss 1.30305 val_loss 1.30305 \n",
            "Epoch 1/10 loss 1.18774 val_loss 1.18774 \n",
            "Epoch 1/10 loss -1.11773 val_loss -1.11773 \n",
            "Epoch 1/10 loss -0.16632 val_loss -0.16632 \n",
            "Epoch 1/10 loss -0.32550 val_loss -0.32550 \n",
            "Epoch 1/10 loss 1.63789 val_loss 1.63789 \n",
            "Epoch 1/10 loss -0.49038 val_loss -0.49038 \n",
            "Epoch 1/10 loss 1.84317 val_loss 1.84317 \n",
            "Epoch 1/10 loss -0.13909 val_loss -0.13909 \n",
            "Epoch 1/10 loss 0.21028 val_loss 0.21028 \n",
            "Epoch 1/10 loss 3.36070 val_loss 3.36070 \n",
            "Epoch 1/10 loss 1.73538 val_loss 1.73538 \n",
            "Epoch 1/10 loss 2.35409 val_loss 2.35409 \n",
            "Epoch 1/10 loss -0.39141 val_loss -0.39141 \n",
            "Epoch 1/10 loss 1.49777 val_loss 1.49777 \n",
            "Epoch 1/10 loss -0.24120 val_loss -0.24120 \n",
            "Epoch 1/10 loss 0.15557 val_loss 0.15557 \n",
            "Epoch 1/10 loss -0.46437 val_loss -0.46437 \n",
            "Epoch 1/10 loss -2.06984 val_loss -2.06984 \n",
            "Epoch 1/10 loss 0.40332 val_loss 0.40332 \n",
            "Epoch 1/10 loss 0.73782 val_loss 0.73782 \n",
            "Epoch 1/10 loss 0.73482 val_loss 0.73482 \n",
            "Epoch 1/10 loss 1.24676 val_loss 1.24676 \n",
            "Epoch 1/10 loss -1.05874 val_loss -1.05874 \n",
            "Epoch 1/10 loss 0.40759 val_loss 0.40759 \n",
            "Epoch 1/10 loss 0.94520 val_loss 0.94520 \n",
            "Epoch 1/10 loss 0.96134 val_loss 0.96134 \n",
            "Epoch 1/10 loss 0.63293 val_loss 0.63293 \n",
            "Epoch 1/10 loss 0.45792 val_loss 0.45792 \n",
            "Epoch 1/10 loss 0.47093 val_loss 0.47093 \n",
            "Epoch 1/10 loss 0.02845 val_loss 0.02845 \n",
            "Epoch 1/10 loss -1.00220 val_loss -1.00220 \n",
            "Epoch 1/10 loss 1.42366 val_loss 1.42366 \n",
            "Epoch 1/10 loss 0.25252 val_loss 0.25252 \n",
            "Epoch 1/10 loss -1.83644 val_loss -1.83644 \n",
            "Epoch 1/10 loss 2.53930 val_loss 2.53930 \n",
            "Epoch 1/10 loss 1.75955 val_loss 1.75955 \n",
            "Epoch 1/10 loss 0.34251 val_loss 0.34251 \n",
            "Epoch 1/10 loss -3.21550 val_loss -3.21550 \n",
            "Epoch 1/10 loss 1.01899 val_loss 1.01899 \n",
            "Epoch 2/10 loss -1.57761 val_loss -1.57761 \n",
            "Epoch 2/10 loss 1.09522 val_loss 1.09522 \n",
            "Epoch 2/10 loss -5.70247 val_loss -5.70247 \n",
            "Epoch 2/10 loss -0.77173 val_loss -0.77173 \n",
            "Epoch 2/10 loss 1.01440 val_loss 1.01440 \n",
            "Epoch 2/10 loss 1.82068 val_loss 1.82068 \n",
            "Epoch 2/10 loss 3.37163 val_loss 3.37163 \n",
            "Epoch 2/10 loss 0.56356 val_loss 0.56356 \n",
            "Epoch 2/10 loss 0.24291 val_loss 0.24291 \n",
            "Epoch 2/10 loss 1.98151 val_loss 1.98151 \n",
            "Epoch 2/10 loss 0.79458 val_loss 0.79458 \n",
            "Epoch 2/10 loss 0.75758 val_loss 0.75758 \n",
            "Epoch 2/10 loss 0.16822 val_loss 0.16822 \n",
            "Epoch 2/10 loss 0.50275 val_loss 0.50275 \n",
            "Epoch 2/10 loss 1.04281 val_loss 1.04281 \n",
            "Epoch 2/10 loss 0.59634 val_loss 0.59634 \n",
            "Epoch 2/10 loss 0.43628 val_loss 0.43628 \n",
            "Epoch 2/10 loss 0.65086 val_loss 0.65086 \n",
            "Epoch 2/10 loss 0.68233 val_loss 0.68233 \n",
            "Epoch 2/10 loss 0.69369 val_loss 0.69369 \n",
            "Epoch 2/10 loss 2.11616 val_loss 2.11616 \n",
            "Epoch 2/10 loss 0.70067 val_loss 0.70067 \n",
            "Epoch 2/10 loss 1.32834 val_loss 1.32834 \n",
            "Epoch 2/10 loss 0.83767 val_loss 0.83767 \n",
            "Epoch 2/10 loss 0.58988 val_loss 0.58988 \n",
            "Epoch 2/10 loss 0.41754 val_loss 0.41754 \n",
            "Epoch 2/10 loss -2.94751 val_loss -2.94751 \n",
            "Epoch 2/10 loss -1.65998 val_loss -1.65998 \n",
            "Epoch 2/10 loss 0.80381 val_loss 0.80381 \n",
            "Epoch 2/10 loss 0.79541 val_loss 0.79541 \n",
            "Epoch 2/10 loss 0.77461 val_loss 0.77461 \n",
            "Epoch 2/10 loss 0.79222 val_loss 0.79222 \n",
            "Epoch 2/10 loss 0.69177 val_loss 0.69177 \n",
            "Epoch 2/10 loss 0.62733 val_loss 0.62733 \n",
            "Epoch 2/10 loss 0.69529 val_loss 0.69529 \n",
            "Epoch 2/10 loss -0.14453 val_loss -0.14453 \n",
            "Epoch 2/10 loss -0.63759 val_loss -0.63759 \n",
            "Epoch 2/10 loss 0.03510 val_loss 0.03510 \n",
            "Epoch 2/10 loss 1.19442 val_loss 1.19442 \n",
            "Epoch 2/10 loss 0.79160 val_loss 0.79160 \n",
            "Epoch 2/10 loss -0.15099 val_loss -0.15099 \n",
            "Epoch 2/10 loss 4.73578 val_loss 4.73578 \n",
            "Epoch 2/10 loss 1.89640 val_loss 1.89640 \n",
            "Epoch 2/10 loss 1.85263 val_loss 1.85263 \n",
            "Epoch 2/10 loss -3.98916 val_loss -3.98916 \n",
            "Epoch 2/10 loss -1.43892 val_loss -1.43892 \n",
            "Epoch 2/10 loss 0.68484 val_loss 0.68484 \n",
            "Epoch 2/10 loss -0.24043 val_loss -0.24043 \n",
            "Epoch 2/10 loss 0.32251 val_loss 0.32251 \n",
            "Epoch 2/10 loss -0.78436 val_loss -0.78436 \n",
            "Epoch 2/10 loss 1.30436 val_loss 1.30436 \n",
            "Epoch 2/10 loss -1.70068 val_loss -1.70068 \n",
            "Epoch 2/10 loss -0.19109 val_loss -0.19109 \n",
            "Epoch 2/10 loss -1.18495 val_loss -1.18495 \n",
            "Epoch 2/10 loss -1.18680 val_loss -1.18680 \n",
            "Epoch 2/10 loss -1.31923 val_loss -1.31923 \n",
            "Epoch 2/10 loss 0.38374 val_loss 0.38374 \n",
            "Epoch 2/10 loss 2.76594 val_loss 2.76594 \n",
            "Epoch 2/10 loss -3.66680 val_loss -3.66680 \n",
            "Epoch 2/10 loss 0.99373 val_loss 0.99373 \n",
            "Epoch 2/10 loss -2.60571 val_loss -2.60571 \n",
            "Epoch 2/10 loss 1.87198 val_loss 1.87198 \n",
            "Epoch 2/10 loss 1.04801 val_loss 1.04801 \n",
            "Epoch 2/10 loss 0.15495 val_loss 0.15495 \n",
            "Epoch 2/10 loss -2.85863 val_loss -2.85863 \n",
            "Epoch 2/10 loss 0.11771 val_loss 0.11771 \n",
            "Epoch 2/10 loss -2.67467 val_loss -2.67467 \n",
            "Epoch 2/10 loss 3.09937 val_loss 3.09937 \n",
            "Epoch 2/10 loss 2.13511 val_loss 2.13511 \n",
            "Epoch 2/10 loss -5.85286 val_loss -5.85286 \n",
            "Epoch 2/10 loss 3.16688 val_loss 3.16688 \n",
            "Epoch 2/10 loss -1.86914 val_loss -1.86914 \n",
            "Epoch 2/10 loss 3.15108 val_loss 3.15108 \n",
            "Epoch 2/10 loss -4.82400 val_loss -4.82400 \n",
            "Epoch 2/10 loss -2.88881 val_loss -2.88881 \n",
            "Epoch 2/10 loss -2.93227 val_loss -2.93227 \n",
            "Epoch 2/10 loss 3.13040 val_loss 3.13040 \n",
            "Epoch 2/10 loss 0.13063 val_loss 0.13063 \n",
            "Epoch 2/10 loss 2.01558 val_loss 2.01558 \n",
            "Epoch 2/10 loss -6.89586 val_loss -6.89586 \n",
            "Epoch 2/10 loss 2.98308 val_loss 2.98308 \n",
            "Epoch 2/10 loss 3.89741 val_loss 3.89741 \n",
            "Epoch 2/10 loss -2.44363 val_loss -2.44363 \n",
            "Epoch 2/10 loss 0.19596 val_loss 0.19596 \n",
            "Epoch 2/10 loss -1.46589 val_loss -1.46589 \n",
            "Epoch 2/10 loss 0.17337 val_loss 0.17337 \n",
            "Epoch 2/10 loss 0.16898 val_loss 0.16898 \n",
            "Epoch 2/10 loss -1.05343 val_loss -1.05343 \n",
            "Epoch 2/10 loss -2.45786 val_loss -2.45786 \n",
            "Epoch 2/10 loss 1.03569 val_loss 1.03569 \n",
            "Epoch 2/10 loss 3.86489 val_loss 3.86489 \n",
            "Epoch 2/10 loss -0.76260 val_loss -0.76260 \n",
            "Epoch 2/10 loss -2.98604 val_loss -2.98604 \n",
            "Epoch 2/10 loss -0.77203 val_loss -0.77203 \n",
            "Epoch 2/10 loss 2.96748 val_loss 2.96748 \n",
            "Epoch 2/10 loss 2.00912 val_loss 2.00912 \n",
            "Epoch 2/10 loss 0.15133 val_loss 0.15133 \n",
            "Epoch 2/10 loss 0.15738 val_loss 0.15738 \n",
            "Epoch 2/10 loss 6.05691 val_loss 6.05691 \n",
            "Epoch 2/10 loss 3.17650 val_loss 3.17650 \n",
            "Epoch 2/10 loss 0.97625 val_loss 0.97625 \n",
            "Epoch 2/10 loss -2.76774 val_loss -2.76774 \n",
            "Epoch 2/10 loss 1.69982 val_loss 1.69982 \n",
            "Epoch 2/10 loss 0.20598 val_loss 0.20598 \n",
            "Epoch 2/10 loss -1.25146 val_loss -1.25146 \n",
            "Epoch 2/10 loss 0.23133 val_loss 0.23133 \n",
            "Epoch 2/10 loss -2.88877 val_loss -2.88877 \n",
            "Epoch 2/10 loss 0.14189 val_loss 0.14189 \n",
            "Epoch 2/10 loss 1.94970 val_loss 1.94970 \n",
            "Epoch 2/10 loss -5.00063 val_loss -5.00063 \n",
            "Epoch 2/10 loss 0.13121 val_loss 0.13121 \n",
            "Epoch 2/10 loss -4.64305 val_loss -4.64305 \n",
            "Epoch 2/10 loss 0.10555 val_loss 0.10555 \n",
            "Epoch 2/10 loss 2.65190 val_loss 2.65190 \n",
            "Epoch 2/10 loss 0.08317 val_loss 0.08317 \n",
            "Epoch 2/10 loss 2.62326 val_loss 2.62326 \n",
            "Epoch 2/10 loss 2.67350 val_loss 2.67350 \n",
            "Epoch 2/10 loss -3.35020 val_loss -3.35020 \n",
            "Epoch 2/10 loss -1.24897 val_loss -1.24897 \n",
            "Epoch 2/10 loss -3.96463 val_loss -3.96463 \n",
            "Epoch 2/10 loss 5.65754 val_loss 5.65754 \n",
            "Epoch 2/10 loss 0.05752 val_loss 0.05752 \n",
            "Epoch 2/10 loss -1.35940 val_loss -1.35940 \n",
            "Epoch 2/10 loss 1.49280 val_loss 1.49280 \n",
            "Epoch 2/10 loss 0.05677 val_loss 0.05677 \n",
            "Epoch 2/10 loss 6.05228 val_loss 6.05228 \n",
            "Epoch 2/10 loss -3.28291 val_loss -3.28291 \n",
            "Epoch 2/10 loss 1.69169 val_loss 1.69169 \n",
            "Epoch 2/10 loss -2.54648 val_loss -2.54648 \n",
            "Epoch 2/10 loss -1.24734 val_loss -1.24734 \n",
            "Epoch 2/10 loss 0.06558 val_loss 0.06558 \n",
            "Epoch 2/10 loss -2.61059 val_loss -2.61059 \n",
            "Epoch 2/10 loss 4.10093 val_loss 4.10093 \n",
            "Epoch 2/10 loss 1.43032 val_loss 1.43032 \n",
            "Epoch 2/10 loss 2.78231 val_loss 2.78231 \n",
            "Epoch 2/10 loss 1.39925 val_loss 1.39925 \n",
            "Epoch 2/10 loss 2.62999 val_loss 2.62999 \n",
            "Epoch 2/10 loss 3.74520 val_loss 3.74520 \n",
            "Epoch 2/10 loss 1.36107 val_loss 1.36107 \n",
            "Epoch 2/10 loss -1.94854 val_loss -1.94854 \n",
            "Epoch 2/10 loss -2.73662 val_loss -2.73662 \n",
            "Epoch 2/10 loss 1.06854 val_loss 1.06854 \n",
            "Epoch 2/10 loss 2.34503 val_loss 2.34503 \n",
            "Epoch 2/10 loss -3.43345 val_loss -3.43345 \n",
            "Epoch 2/10 loss 0.14321 val_loss 0.14321 \n",
            "Epoch 2/10 loss -4.15891 val_loss -4.15891 \n",
            "Epoch 2/10 loss 1.17319 val_loss 1.17319 \n",
            "Epoch 2/10 loss -1.01988 val_loss -1.01988 \n",
            "Epoch 2/10 loss 4.43762 val_loss 4.43762 \n",
            "Epoch 2/10 loss -5.40091 val_loss -5.40091 \n",
            "Epoch 3/10 loss 0.07749 val_loss 0.07749 \n",
            "Epoch 3/10 loss -3.78867 val_loss -3.78867 \n",
            "Epoch 3/10 loss 1.41613 val_loss 1.41613 \n",
            "Epoch 3/10 loss -2.07155 val_loss -2.07155 \n",
            "Epoch 3/10 loss 4.37928 val_loss 4.37928 \n",
            "Epoch 3/10 loss 0.17700 val_loss 0.17700 \n",
            "Epoch 3/10 loss -1.56481 val_loss -1.56481 \n",
            "Epoch 3/10 loss 0.04981 val_loss 0.04981 \n",
            "Epoch 3/10 loss 6.06143 val_loss 6.06143 \n",
            "Epoch 3/10 loss 0.05032 val_loss 0.05032 \n",
            "Epoch 3/10 loss 1.35787 val_loss 1.35787 \n",
            "Epoch 3/10 loss 0.80016 val_loss 0.80016 \n",
            "Epoch 3/10 loss 3.87127 val_loss 3.87127 \n",
            "Epoch 3/10 loss -2.10091 val_loss -2.10091 \n",
            "Epoch 3/10 loss 0.10327 val_loss 0.10327 \n",
            "Epoch 3/10 loss 2.34482 val_loss 2.34482 \n",
            "Epoch 3/10 loss -0.93043 val_loss -0.93043 \n",
            "Epoch 3/10 loss 0.12774 val_loss 0.12774 \n",
            "Epoch 3/10 loss -3.67756 val_loss -3.67756 \n",
            "Epoch 3/10 loss -5.21288 val_loss -5.21288 \n",
            "Epoch 3/10 loss -3.12900 val_loss -3.12900 \n",
            "Epoch 3/10 loss -0.01218 val_loss -0.01218 \n",
            "Epoch 3/10 loss 4.22606 val_loss 4.22606 \n",
            "Epoch 3/10 loss 7.02754 val_loss 7.02754 \n",
            "Epoch 3/10 loss 0.13178 val_loss 0.13178 \n",
            "Epoch 3/10 loss 0.13190 val_loss 0.13190 \n",
            "Epoch 3/10 loss 2.04691 val_loss 2.04691 \n",
            "Epoch 3/10 loss -2.68997 val_loss -2.68997 \n",
            "Epoch 3/10 loss -2.08402 val_loss -2.08402 \n",
            "Epoch 3/10 loss -0.83422 val_loss -0.83422 \n",
            "Epoch 3/10 loss 2.11444 val_loss 2.11444 \n",
            "Epoch 3/10 loss 3.28335 val_loss 3.28335 \n",
            "Epoch 3/10 loss 3.38573 val_loss 3.38573 \n",
            "Epoch 3/10 loss 1.09091 val_loss 1.09091 \n",
            "Epoch 3/10 loss -1.67320 val_loss -1.67320 \n",
            "Epoch 3/10 loss 2.37362 val_loss 2.37362 \n",
            "Epoch 3/10 loss 2.82277 val_loss 2.82277 \n",
            "Epoch 3/10 loss 2.72079 val_loss 2.72079 \n",
            "Epoch 3/10 loss 1.76555 val_loss 1.76555 \n",
            "Epoch 3/10 loss 2.36966 val_loss 2.36966 \n",
            "Epoch 3/10 loss 0.00341 val_loss 0.00341 \n",
            "Epoch 3/10 loss 1.90899 val_loss 1.90899 \n",
            "Epoch 3/10 loss -1.39472 val_loss -1.39472 \n",
            "Epoch 3/10 loss -3.16872 val_loss -3.16872 \n",
            "Epoch 3/10 loss -0.40576 val_loss -0.40576 \n",
            "Epoch 3/10 loss -4.57967 val_loss -4.57967 \n",
            "Epoch 3/10 loss -0.55568 val_loss -0.55568 \n",
            "Epoch 3/10 loss 0.81590 val_loss 0.81590 \n",
            "Epoch 3/10 loss 1.39154 val_loss 1.39154 \n",
            "Epoch 3/10 loss 2.59093 val_loss 2.59093 \n",
            "Epoch 3/10 loss -1.42311 val_loss -1.42311 \n",
            "Epoch 3/10 loss 1.45080 val_loss 1.45080 \n",
            "Epoch 3/10 loss 1.46097 val_loss 1.46097 \n",
            "Epoch 3/10 loss 1.44613 val_loss 1.44613 \n",
            "Epoch 3/10 loss -0.84926 val_loss -0.84926 \n",
            "Epoch 3/10 loss -0.29006 val_loss -0.29006 \n",
            "Epoch 3/10 loss -0.36457 val_loss -0.36457 \n",
            "Epoch 3/10 loss 2.57292 val_loss 2.57292 \n",
            "Epoch 3/10 loss 1.35491 val_loss 1.35491 \n",
            "Epoch 3/10 loss 0.32858 val_loss 0.32858 \n",
            "Epoch 3/10 loss 0.35483 val_loss 0.35483 \n",
            "Epoch 3/10 loss 1.55234 val_loss 1.55234 \n",
            "Epoch 3/10 loss 1.76049 val_loss 1.76049 \n",
            "Epoch 3/10 loss 0.47068 val_loss 0.47068 \n",
            "Epoch 3/10 loss 0.64083 val_loss 0.64083 \n",
            "Epoch 3/10 loss -2.24446 val_loss -2.24446 \n",
            "Epoch 3/10 loss 0.92940 val_loss 0.92940 \n",
            "Epoch 3/10 loss -1.44358 val_loss -1.44358 \n",
            "Epoch 3/10 loss -4.06695 val_loss -4.06695 \n",
            "Epoch 3/10 loss 0.66938 val_loss 0.66938 \n",
            "Epoch 3/10 loss 4.77269 val_loss 4.77269 \n",
            "Epoch 3/10 loss 0.69756 val_loss 0.69756 \n",
            "Epoch 3/10 loss 0.82488 val_loss 0.82488 \n",
            "Epoch 3/10 loss 0.70276 val_loss 0.70276 \n",
            "Epoch 3/10 loss 1.08118 val_loss 1.08118 \n",
            "Epoch 3/10 loss 0.97382 val_loss 0.97382 \n",
            "Epoch 3/10 loss 0.40431 val_loss 0.40431 \n",
            "Epoch 3/10 loss -0.28970 val_loss -0.28970 \n",
            "Epoch 3/10 loss 0.54527 val_loss 0.54527 \n",
            "Epoch 3/10 loss 0.69627 val_loss 0.69627 \n",
            "Epoch 3/10 loss 0.61513 val_loss 0.61513 \n",
            "Epoch 3/10 loss 0.69753 val_loss 0.69753 \n",
            "Epoch 3/10 loss 0.90183 val_loss 0.90183 \n",
            "Epoch 3/10 loss 0.60796 val_loss 0.60796 \n",
            "Epoch 3/10 loss -1.75191 val_loss -1.75191 \n",
            "Epoch 3/10 loss 0.75467 val_loss 0.75467 \n",
            "Epoch 3/10 loss -1.31162 val_loss -1.31162 \n",
            "Epoch 3/10 loss 0.70461 val_loss 0.70461 \n",
            "Epoch 3/10 loss 0.70905 val_loss 0.70905 \n",
            "Epoch 3/10 loss -1.71605 val_loss -1.71605 \n",
            "Epoch 3/10 loss -2.39268 val_loss -2.39268 \n",
            "Epoch 3/10 loss 0.73328 val_loss 0.73328 \n",
            "Epoch 3/10 loss -1.86113 val_loss -1.86113 \n",
            "Epoch 3/10 loss 0.22984 val_loss 0.22984 \n",
            "Epoch 3/10 loss 0.69780 val_loss 0.69780 \n",
            "Epoch 3/10 loss 0.42904 val_loss 0.42904 \n",
            "Epoch 3/10 loss 0.13421 val_loss 0.13421 \n",
            "Epoch 3/10 loss 1.33005 val_loss 1.33005 \n",
            "Epoch 3/10 loss 1.70011 val_loss 1.70011 \n",
            "Epoch 3/10 loss 0.74538 val_loss 0.74538 \n",
            "Epoch 3/10 loss -0.24835 val_loss -0.24835 \n",
            "Epoch 3/10 loss 0.39453 val_loss 0.39453 \n",
            "Epoch 3/10 loss 0.37229 val_loss 0.37229 \n",
            "Epoch 3/10 loss 0.78501 val_loss 0.78501 \n",
            "Epoch 3/10 loss 1.26237 val_loss 1.26237 \n",
            "Epoch 3/10 loss 0.73059 val_loss 0.73059 \n",
            "Epoch 3/10 loss -0.15585 val_loss -0.15585 \n",
            "Epoch 3/10 loss 0.31472 val_loss 0.31472 \n",
            "Epoch 3/10 loss -0.21549 val_loss -0.21549 \n",
            "Epoch 3/10 loss 1.39143 val_loss 1.39143 \n",
            "Epoch 3/10 loss -2.15483 val_loss -2.15483 \n",
            "Epoch 3/10 loss -1.49460 val_loss -1.49460 \n",
            "Epoch 3/10 loss 1.51624 val_loss 1.51624 \n",
            "Epoch 3/10 loss -2.41631 val_loss -2.41631 \n",
            "Epoch 3/10 loss 1.67183 val_loss 1.67183 \n",
            "Epoch 3/10 loss -2.14261 val_loss -2.14261 \n",
            "Epoch 3/10 loss -0.68502 val_loss -0.68502 \n",
            "Epoch 3/10 loss 2.93982 val_loss 2.93982 \n",
            "Epoch 3/10 loss 2.08496 val_loss 2.08496 \n",
            "Epoch 3/10 loss -1.87210 val_loss -1.87210 \n",
            "Epoch 3/10 loss 2.20137 val_loss 2.20137 \n",
            "Epoch 3/10 loss 1.17580 val_loss 1.17580 \n",
            "Epoch 3/10 loss -0.96415 val_loss -0.96415 \n",
            "Epoch 3/10 loss 4.49190 val_loss 4.49190 \n",
            "Epoch 3/10 loss -2.06961 val_loss -2.06961 \n",
            "Epoch 3/10 loss 2.27600 val_loss 2.27600 \n",
            "Epoch 3/10 loss -4.39496 val_loss -4.39496 \n",
            "Epoch 3/10 loss -0.96924 val_loss -0.96924 \n",
            "Epoch 3/10 loss -4.18708 val_loss -4.18708 \n",
            "Epoch 3/10 loss 0.10226 val_loss 0.10226 \n",
            "Epoch 3/10 loss -7.98545 val_loss -7.98545 \n",
            "Epoch 3/10 loss -5.66743 val_loss -5.66743 \n",
            "Epoch 3/10 loss -4.86972 val_loss -4.86972 \n",
            "Epoch 3/10 loss -1.26863 val_loss -1.26863 \n",
            "Epoch 3/10 loss -2.88487 val_loss -2.88487 \n",
            "Epoch 3/10 loss 2.41992 val_loss 2.41992 \n",
            "Epoch 3/10 loss 4.97700 val_loss 4.97700 \n",
            "Epoch 3/10 loss 0.03450 val_loss 0.03450 \n",
            "Epoch 3/10 loss 5.11759 val_loss 5.11759 \n",
            "Epoch 3/10 loss -3.33466 val_loss -3.33466 \n",
            "Epoch 3/10 loss -11.51400 val_loss -11.51400 \n",
            "Epoch 3/10 loss 1.76816 val_loss 1.76816 \n",
            "Epoch 3/10 loss -0.56948 val_loss -0.56948 \n",
            "Epoch 3/10 loss -5.63457 val_loss -5.63457 \n",
            "Epoch 3/10 loss -3.77643 val_loss -3.77643 \n",
            "Epoch 3/10 loss 7.80237 val_loss 7.80237 \n",
            "Epoch 3/10 loss 0.00795 val_loss 0.00795 \n",
            "Epoch 3/10 loss -6.89982 val_loss -6.89982 \n",
            "Epoch 3/10 loss -4.66596 val_loss -4.66596 \n",
            "Epoch 3/10 loss -7.81407 val_loss -7.81407 \n",
            "Epoch 4/10 loss 3.05899 val_loss 3.05899 \n",
            "Epoch 4/10 loss -0.40760 val_loss -0.40760 \n",
            "Epoch 4/10 loss 7.46236 val_loss 7.46236 \n",
            "Epoch 4/10 loss -8.32137 val_loss -8.32137 \n",
            "Epoch 4/10 loss 6.90949 val_loss 6.90949 \n",
            "Epoch 4/10 loss 5.07470 val_loss 5.07470 \n",
            "Epoch 4/10 loss -0.50442 val_loss -0.50442 \n",
            "Epoch 4/10 loss -9.99327 val_loss -9.99327 \n",
            "Epoch 4/10 loss 0.01157 val_loss 0.01157 \n",
            "Epoch 4/10 loss 5.04957 val_loss 5.04957 \n",
            "Epoch 4/10 loss 5.23804 val_loss 5.23804 \n",
            "Epoch 4/10 loss 8.99090 val_loss 8.99090 \n",
            "Epoch 4/10 loss 6.43811 val_loss 6.43811 \n",
            "Epoch 4/10 loss 2.37123 val_loss 2.37123 \n",
            "Epoch 4/10 loss -3.94246 val_loss -3.94246 \n",
            "Epoch 4/10 loss 3.82670 val_loss 3.82670 \n",
            "Epoch 4/10 loss -1.28993 val_loss -1.28993 \n",
            "Epoch 4/10 loss -3.43186 val_loss -3.43186 \n",
            "Epoch 4/10 loss -6.76379 val_loss -6.76379 \n",
            "Epoch 4/10 loss -7.25860 val_loss -7.25860 \n",
            "Epoch 4/10 loss 5.61867 val_loss 5.61867 \n",
            "Epoch 4/10 loss -1.84904 val_loss -1.84904 \n",
            "Epoch 4/10 loss -1.91511 val_loss -1.91511 \n",
            "Epoch 4/10 loss 3.83168 val_loss 3.83168 \n",
            "Epoch 4/10 loss 3.82104 val_loss 3.82104 \n",
            "Epoch 4/10 loss 3.80415 val_loss 3.80415 \n",
            "Epoch 4/10 loss 1.85937 val_loss 1.85937 \n",
            "Epoch 4/10 loss -3.54798 val_loss -3.54798 \n",
            "Epoch 4/10 loss 7.09549 val_loss 7.09549 \n",
            "Epoch 4/10 loss -3.38448 val_loss -3.38448 \n",
            "Epoch 4/10 loss 1.71089 val_loss 1.71089 \n",
            "Epoch 4/10 loss 3.29618 val_loss 3.29618 \n",
            "Epoch 4/10 loss 0.04120 val_loss 0.04120 \n",
            "Epoch 4/10 loss -0.97750 val_loss -0.97750 \n",
            "Epoch 4/10 loss -2.94486 val_loss -2.94486 \n",
            "Epoch 4/10 loss -5.77729 val_loss -5.77729 \n",
            "Epoch 4/10 loss -3.74285 val_loss -3.74285 \n",
            "Epoch 4/10 loss -2.94280 val_loss -2.94280 \n",
            "Epoch 4/10 loss -5.00178 val_loss -5.00178 \n",
            "Epoch 4/10 loss 0.04193 val_loss 0.04193 \n",
            "Epoch 4/10 loss 4.86596 val_loss 4.86596 \n",
            "Epoch 4/10 loss -12.40575 val_loss -12.40575 \n",
            "Epoch 4/10 loss 1.67952 val_loss 1.67952 \n",
            "Epoch 4/10 loss -3.28870 val_loss -3.28870 \n",
            "Epoch 4/10 loss 0.03281 val_loss 0.03281 \n",
            "Epoch 4/10 loss 1.76468 val_loss 1.76468 \n",
            "Epoch 4/10 loss 2.56461 val_loss 2.56461 \n",
            "Epoch 4/10 loss 0.02819 val_loss 0.02819 \n",
            "Epoch 4/10 loss 0.02727 val_loss 0.02727 \n",
            "Epoch 4/10 loss -2.90702 val_loss -2.90702 \n",
            "Epoch 4/10 loss 1.87013 val_loss 1.87013 \n",
            "Epoch 4/10 loss 3.05233 val_loss 3.05233 \n",
            "Epoch 4/10 loss 1.87794 val_loss 1.87794 \n",
            "Epoch 4/10 loss -0.15195 val_loss -0.15195 \n",
            "Epoch 4/10 loss 0.02657 val_loss 0.02657 \n",
            "Epoch 4/10 loss -1.75327 val_loss -1.75327 \n",
            "Epoch 4/10 loss 0.02858 val_loss 0.02858 \n",
            "Epoch 4/10 loss 5.30932 val_loss 5.30932 \n",
            "Epoch 4/10 loss 3.47477 val_loss 3.47477 \n",
            "Epoch 4/10 loss 1.69946 val_loss 1.69946 \n",
            "Epoch 4/10 loss 4.69795 val_loss 4.69795 \n",
            "Epoch 4/10 loss 0.05968 val_loss 0.05968 \n",
            "Epoch 4/10 loss 10.44637 val_loss 10.44637 \n",
            "Epoch 4/10 loss 0.10761 val_loss 0.10761 \n",
            "Epoch 4/10 loss -1.57012 val_loss -1.57012 \n",
            "Epoch 4/10 loss -8.25927 val_loss -8.25927 \n",
            "Epoch 4/10 loss 0.97044 val_loss 0.97044 \n",
            "Epoch 4/10 loss 0.85875 val_loss 0.85875 \n",
            "Epoch 4/10 loss 0.21929 val_loss 0.21929 \n",
            "Epoch 4/10 loss 1.31975 val_loss 1.31975 \n",
            "Epoch 4/10 loss -1.03190 val_loss -1.03190 \n",
            "Epoch 4/10 loss -0.94212 val_loss -0.94212 \n",
            "Epoch 4/10 loss -1.33692 val_loss -1.33692 \n",
            "Epoch 4/10 loss 1.87155 val_loss 1.87155 \n",
            "Epoch 4/10 loss 0.79516 val_loss 0.79516 \n",
            "Epoch 4/10 loss -0.20481 val_loss -0.20481 \n",
            "Epoch 4/10 loss 1.11608 val_loss 1.11608 \n",
            "Epoch 4/10 loss 1.04345 val_loss 1.04345 \n",
            "Epoch 4/10 loss 0.48254 val_loss 0.48254 \n",
            "Epoch 4/10 loss 0.88569 val_loss 0.88569 \n",
            "Epoch 4/10 loss 0.24129 val_loss 0.24129 \n",
            "Epoch 4/10 loss 0.69749 val_loss 0.69749 \n",
            "Epoch 4/10 loss -3.74185 val_loss -3.74185 \n",
            "Epoch 4/10 loss 0.71081 val_loss 0.71081 \n",
            "Epoch 4/10 loss 0.28125 val_loss 0.28125 \n",
            "Epoch 4/10 loss 0.19258 val_loss 0.19258 \n",
            "Epoch 4/10 loss -6.09177 val_loss -6.09177 \n",
            "Epoch 4/10 loss -3.81588 val_loss -3.81588 \n",
            "Epoch 4/10 loss 1.31657 val_loss 1.31657 \n",
            "Epoch 4/10 loss 2.30363 val_loss 2.30363 \n",
            "Epoch 4/10 loss 2.45198 val_loss 2.45198 \n",
            "Epoch 4/10 loss -1.33185 val_loss -1.33185 \n",
            "Epoch 4/10 loss 2.66223 val_loss 2.66223 \n",
            "Epoch 4/10 loss 1.02662 val_loss 1.02662 \n",
            "Epoch 4/10 loss 3.75948 val_loss 3.75948 \n",
            "Epoch 4/10 loss 1.93094 val_loss 1.93094 \n",
            "Epoch 4/10 loss 0.16771 val_loss 0.16771 \n",
            "Epoch 4/10 loss -3.10905 val_loss -3.10905 \n",
            "Epoch 4/10 loss -0.68591 val_loss -0.68591 \n",
            "Epoch 4/10 loss 3.74598 val_loss 3.74598 \n",
            "Epoch 4/10 loss 0.15448 val_loss 0.15448 \n",
            "Epoch 4/10 loss -0.73020 val_loss -0.73020 \n",
            "Epoch 4/10 loss -0.74851 val_loss -0.74851 \n",
            "Epoch 4/10 loss -3.58797 val_loss -3.58797 \n",
            "Epoch 4/10 loss 8.33413 val_loss 8.33413 \n",
            "Epoch 4/10 loss -0.83474 val_loss -0.83474 \n",
            "Epoch 4/10 loss 0.27818 val_loss 0.27818 \n",
            "Epoch 4/10 loss -2.74678 val_loss -2.74678 \n",
            "Epoch 4/10 loss 2.09434 val_loss 2.09434 \n",
            "Epoch 4/10 loss 0.25720 val_loss 0.25720 \n",
            "Epoch 4/10 loss -5.28800 val_loss -5.28800 \n",
            "Epoch 4/10 loss -0.75097 val_loss -0.75097 \n",
            "Epoch 4/10 loss -5.27308 val_loss -5.27308 \n",
            "Epoch 4/10 loss -2.68041 val_loss -2.68041 \n",
            "Epoch 4/10 loss 3.89984 val_loss 3.89984 \n",
            "Epoch 4/10 loss 2.89502 val_loss 2.89502 \n",
            "Epoch 4/10 loss 1.88462 val_loss 1.88462 \n",
            "Epoch 4/10 loss 2.55515 val_loss 2.55515 \n",
            "Epoch 4/10 loss 0.22317 val_loss 0.22317 \n",
            "Epoch 4/10 loss 0.86936 val_loss 0.86936 \n",
            "Epoch 4/10 loss -0.76505 val_loss -0.76505 \n",
            "Epoch 4/10 loss 1.29851 val_loss 1.29851 \n",
            "Epoch 4/10 loss 0.34993 val_loss 0.34993 \n",
            "Epoch 4/10 loss -1.19616 val_loss -1.19616 \n",
            "Epoch 4/10 loss -0.04774 val_loss -0.04774 \n",
            "Epoch 4/10 loss 1.23847 val_loss 1.23847 \n",
            "Epoch 4/10 loss 0.33265 val_loss 0.33265 \n",
            "Epoch 4/10 loss 1.13464 val_loss 1.13464 \n",
            "Epoch 4/10 loss -1.19519 val_loss -1.19519 \n",
            "Epoch 4/10 loss 1.23736 val_loss 1.23736 \n",
            "Epoch 4/10 loss 2.20350 val_loss 2.20350 \n",
            "Epoch 4/10 loss -0.38231 val_loss -0.38231 \n",
            "Epoch 4/10 loss -2.22362 val_loss -2.22362 \n",
            "Epoch 4/10 loss -0.03999 val_loss -0.03999 \n",
            "Epoch 4/10 loss -0.51501 val_loss -0.51501 \n",
            "Epoch 4/10 loss -4.11280 val_loss -4.11280 \n",
            "Epoch 4/10 loss -3.13379 val_loss -3.13379 \n",
            "Epoch 4/10 loss 0.86417 val_loss 0.86417 \n",
            "Epoch 4/10 loss 2.19386 val_loss 2.19386 \n",
            "Epoch 4/10 loss 1.56419 val_loss 1.56419 \n",
            "Epoch 4/10 loss -0.42053 val_loss -0.42053 \n",
            "Epoch 4/10 loss -9.00904 val_loss -9.00904 \n",
            "Epoch 4/10 loss 1.34203 val_loss 1.34203 \n",
            "Epoch 4/10 loss -1.78738 val_loss -1.78738 \n",
            "Epoch 4/10 loss -2.56843 val_loss -2.56843 \n",
            "Epoch 4/10 loss -2.89228 val_loss -2.89228 \n",
            "Epoch 4/10 loss -2.47664 val_loss -2.47664 \n",
            "Epoch 4/10 loss -0.86640 val_loss -0.86640 \n",
            "Epoch 4/10 loss -1.00101 val_loss -1.00101 \n",
            "Epoch 4/10 loss -1.53382 val_loss -1.53382 \n",
            "Epoch 5/10 loss 3.99963 val_loss 3.99963 \n",
            "Epoch 5/10 loss 4.74518 val_loss 4.74518 \n",
            "Epoch 5/10 loss -5.50089 val_loss -5.50089 \n",
            "Epoch 5/10 loss 1.51854 val_loss 1.51854 \n",
            "Epoch 5/10 loss -6.05403 val_loss -6.05403 \n",
            "Epoch 5/10 loss 2.04482 val_loss 2.04482 \n",
            "Epoch 5/10 loss -5.00266 val_loss -5.00266 \n",
            "Epoch 5/10 loss -4.41750 val_loss -4.41750 \n",
            "Epoch 5/10 loss 2.83149 val_loss 2.83149 \n",
            "Epoch 5/10 loss -1.88670 val_loss -1.88670 \n",
            "Epoch 5/10 loss 1.92883 val_loss 1.92883 \n",
            "Epoch 5/10 loss 0.01718 val_loss 0.01718 \n",
            "Epoch 5/10 loss 2.08378 val_loss 2.08378 \n",
            "Epoch 5/10 loss 0.17714 val_loss 0.17714 \n",
            "Epoch 5/10 loss 14.12480 val_loss 14.12480 \n",
            "Epoch 5/10 loss -3.29548 val_loss -3.29548 \n",
            "Epoch 5/10 loss 6.29229 val_loss 6.29229 \n",
            "Epoch 5/10 loss -4.12252 val_loss -4.12252 \n",
            "Epoch 5/10 loss -2.04716 val_loss -2.04716 \n",
            "Epoch 5/10 loss 8.31081 val_loss 8.31081 \n",
            "Epoch 5/10 loss -8.93304 val_loss -8.93304 \n",
            "Epoch 5/10 loss 6.13672 val_loss 6.13672 \n",
            "Epoch 5/10 loss -8.01522 val_loss -8.01522 \n",
            "Epoch 5/10 loss 8.11892 val_loss 8.11892 \n",
            "Epoch 5/10 loss 0.01824 val_loss 0.01824 \n",
            "Epoch 5/10 loss 3.08160 val_loss 3.08160 \n",
            "Epoch 5/10 loss -1.89045 val_loss -1.89045 \n",
            "Epoch 5/10 loss 5.63762 val_loss 5.63762 \n",
            "Epoch 5/10 loss -13.90551 val_loss -13.90551 \n",
            "Epoch 5/10 loss 2.96071 val_loss 2.96071 \n",
            "Epoch 5/10 loss -5.18417 val_loss -5.18417 \n",
            "Epoch 5/10 loss -1.12175 val_loss -1.12175 \n",
            "Epoch 5/10 loss 5.22644 val_loss 5.22644 \n",
            "Epoch 5/10 loss -9.06811 val_loss -9.06811 \n",
            "Epoch 5/10 loss -1.62084 val_loss -1.62084 \n",
            "Epoch 5/10 loss -9.07639 val_loss -9.07639 \n",
            "Epoch 5/10 loss -1.66171 val_loss -1.66171 \n",
            "Epoch 5/10 loss -6.86886 val_loss -6.86886 \n",
            "Epoch 5/10 loss 5.41746 val_loss 5.41746 \n",
            "Epoch 5/10 loss -1.80402 val_loss -1.80402 \n",
            "Epoch 5/10 loss 0.02356 val_loss 0.02356 \n",
            "Epoch 5/10 loss 0.02180 val_loss 0.02180 \n",
            "Epoch 5/10 loss -8.09812 val_loss -8.09812 \n",
            "Epoch 5/10 loss 6.02871 val_loss 6.02871 \n",
            "Epoch 5/10 loss -0.87542 val_loss -0.87542 \n",
            "Epoch 5/10 loss -4.05892 val_loss -4.05892 \n",
            "Epoch 5/10 loss 1.22968 val_loss 1.22968 \n",
            "Epoch 5/10 loss 6.23720 val_loss 6.23720 \n",
            "Epoch 5/10 loss -6.11989 val_loss -6.11989 \n",
            "Epoch 5/10 loss -5.85097 val_loss -5.85097 \n",
            "Epoch 5/10 loss -4.10486 val_loss -4.10486 \n",
            "Epoch 5/10 loss -4.09848 val_loss -4.09848 \n",
            "Epoch 5/10 loss 0.01384 val_loss 0.01384 \n",
            "Epoch 5/10 loss 4.36841 val_loss 4.36841 \n",
            "Epoch 5/10 loss 2.20646 val_loss 2.20646 \n",
            "Epoch 5/10 loss 3.12748 val_loss 3.12748 \n",
            "Epoch 5/10 loss -5.14586 val_loss -5.14586 \n",
            "Epoch 5/10 loss 8.85701 val_loss 8.85701 \n",
            "Epoch 5/10 loss -2.16485 val_loss -2.16485 \n",
            "Epoch 5/10 loss -6.46327 val_loss -6.46327 \n",
            "Epoch 5/10 loss -2.16188 val_loss -2.16188 \n",
            "Epoch 5/10 loss -2.18826 val_loss -2.18826 \n",
            "Epoch 5/10 loss 2.24579 val_loss 2.24579 \n",
            "Epoch 5/10 loss 4.51523 val_loss 4.51523 \n",
            "Epoch 5/10 loss 9.01634 val_loss 9.01634 \n",
            "Epoch 5/10 loss 0.01202 val_loss 0.01202 \n",
            "Epoch 5/10 loss -4.31308 val_loss -4.31308 \n",
            "Epoch 5/10 loss 4.37673 val_loss 4.37673 \n",
            "Epoch 5/10 loss 2.12527 val_loss 2.12527 \n",
            "Epoch 5/10 loss 2.08272 val_loss 2.08272 \n",
            "Epoch 5/10 loss -0.96067 val_loss -0.96067 \n",
            "Epoch 5/10 loss -5.96672 val_loss -5.96672 \n",
            "Epoch 5/10 loss 1.93272 val_loss 1.93272 \n",
            "Epoch 5/10 loss 3.73539 val_loss 3.73539 \n",
            "Epoch 5/10 loss 1.81220 val_loss 1.81220 \n",
            "Epoch 5/10 loss 0.03211 val_loss 0.03211 \n",
            "Epoch 5/10 loss 1.68048 val_loss 1.68048 \n",
            "Epoch 5/10 loss 0.04195 val_loss 0.04195 \n",
            "Epoch 5/10 loss 0.04741 val_loss 0.04741 \n",
            "Epoch 5/10 loss -1.40440 val_loss -1.40440 \n",
            "Epoch 5/10 loss 2.89678 val_loss 2.89678 \n",
            "Epoch 5/10 loss 5.22439 val_loss 5.22439 \n",
            "Epoch 5/10 loss 0.81659 val_loss 0.81659 \n",
            "Epoch 5/10 loss -10.41137 val_loss -10.41137 \n",
            "Epoch 5/10 loss -1.23011 val_loss -1.23011 \n",
            "Epoch 5/10 loss -5.57592 val_loss -5.57592 \n",
            "Epoch 5/10 loss 4.02251 val_loss 4.02251 \n",
            "Epoch 5/10 loss -4.96626 val_loss -4.96626 \n",
            "Epoch 5/10 loss 0.06978 val_loss 0.06978 \n",
            "Epoch 5/10 loss 4.04429 val_loss 4.04429 \n",
            "Epoch 5/10 loss 1.37499 val_loss 1.37499 \n",
            "Epoch 5/10 loss 5.17426 val_loss 5.17426 \n",
            "Epoch 5/10 loss -4.74259 val_loss -4.74259 \n",
            "Epoch 5/10 loss -7.81892 val_loss -7.81892 \n",
            "Epoch 5/10 loss 0.04387 val_loss 0.04387 \n",
            "Epoch 5/10 loss -2.99907 val_loss -2.99907 \n",
            "Epoch 5/10 loss 0.08473 val_loss 0.08473 \n",
            "Epoch 5/10 loss -3.64042 val_loss -3.64042 \n",
            "Epoch 5/10 loss -4.31450 val_loss -4.31450 \n",
            "Epoch 5/10 loss -1.19450 val_loss -1.19450 \n",
            "Epoch 5/10 loss -2.54590 val_loss -2.54590 \n",
            "Epoch 5/10 loss 0.06309 val_loss 0.06309 \n",
            "Epoch 5/10 loss 4.31172 val_loss 4.31172 \n",
            "Epoch 5/10 loss 2.92413 val_loss 2.92413 \n",
            "Epoch 5/10 loss -1.62016 val_loss -1.62016 \n",
            "Epoch 5/10 loss 4.29685 val_loss 4.29685 \n",
            "Epoch 5/10 loss -2.67304 val_loss -2.67304 \n",
            "Epoch 5/10 loss 2.76546 val_loss 2.76546 \n",
            "Epoch 5/10 loss 4.01274 val_loss 4.01274 \n",
            "Epoch 5/10 loss -9.89695 val_loss -9.89695 \n",
            "Epoch 5/10 loss 4.98951 val_loss 4.98951 \n",
            "Epoch 5/10 loss 1.25693 val_loss 1.25693 \n",
            "Epoch 5/10 loss 3.40045 val_loss 3.40045 \n",
            "Epoch 5/10 loss 2.13881 val_loss 2.13881 \n",
            "Epoch 5/10 loss 1.92171 val_loss 1.92171 \n",
            "Epoch 5/10 loss -2.32919 val_loss -2.32919 \n",
            "Epoch 5/10 loss 0.24559 val_loss 0.24559 \n",
            "Epoch 5/10 loss 1.38664 val_loss 1.38664 \n",
            "Epoch 5/10 loss -0.65842 val_loss -0.65842 \n",
            "Epoch 5/10 loss 1.55758 val_loss 1.55758 \n",
            "Epoch 5/10 loss 1.68285 val_loss 1.68285 \n",
            "Epoch 5/10 loss 1.53807 val_loss 1.53807 \n",
            "Epoch 5/10 loss 0.52692 val_loss 0.52692 \n",
            "Epoch 5/10 loss 0.69474 val_loss 0.69474 \n",
            "Epoch 5/10 loss 0.59965 val_loss 0.59965 \n",
            "Epoch 5/10 loss 0.91882 val_loss 0.91882 \n",
            "Epoch 5/10 loss 0.99059 val_loss 0.99059 \n",
            "Epoch 5/10 loss 1.46150 val_loss 1.46150 \n",
            "Epoch 5/10 loss 0.80353 val_loss 0.80353 \n",
            "Epoch 5/10 loss 0.69441 val_loss 0.69441 \n",
            "Epoch 5/10 loss 0.69310 val_loss 0.69310 \n",
            "Epoch 5/10 loss -2.55990 val_loss -2.55990 \n",
            "Epoch 5/10 loss 0.76268 val_loss 0.76268 \n",
            "Epoch 5/10 loss -2.86208 val_loss -2.86208 \n",
            "Epoch 5/10 loss 0.29566 val_loss 0.29566 \n",
            "Epoch 5/10 loss -2.71335 val_loss -2.71335 \n",
            "Epoch 5/10 loss -2.54874 val_loss -2.54874 \n",
            "Epoch 5/10 loss 0.24382 val_loss 0.24382 \n",
            "Epoch 5/10 loss 0.45033 val_loss 0.45033 \n",
            "Epoch 5/10 loss 0.41779 val_loss 0.41779 \n",
            "Epoch 5/10 loss -1.12249 val_loss -1.12249 \n",
            "Epoch 5/10 loss 0.79892 val_loss 0.79892 \n",
            "Epoch 5/10 loss 1.92652 val_loss 1.92652 \n",
            "Epoch 5/10 loss 2.03767 val_loss 2.03767 \n",
            "Epoch 5/10 loss 0.26151 val_loss 0.26151 \n",
            "Epoch 5/10 loss 2.73781 val_loss 2.73781 \n",
            "Epoch 5/10 loss -1.53761 val_loss -1.53761 \n",
            "Epoch 5/10 loss 0.74559 val_loss 0.74559 \n",
            "Epoch 5/10 loss 0.25239 val_loss 0.25239 \n",
            "Epoch 5/10 loss 0.24756 val_loss 0.24756 \n",
            "Epoch 6/10 loss 2.83842 val_loss 2.83842 \n",
            "Epoch 6/10 loss -3.12154 val_loss -3.12154 \n",
            "Epoch 6/10 loss -3.65936 val_loss -3.65936 \n",
            "Epoch 6/10 loss -1.92107 val_loss -1.92107 \n",
            "Epoch 6/10 loss -6.45057 val_loss -6.45057 \n",
            "Epoch 6/10 loss -1.40054 val_loss -1.40054 \n",
            "Epoch 6/10 loss -1.97875 val_loss -1.97875 \n",
            "Epoch 6/10 loss 8.27439 val_loss 8.27439 \n",
            "Epoch 6/10 loss -0.80125 val_loss -0.80125 \n",
            "Epoch 6/10 loss 0.83430 val_loss 0.83430 \n",
            "Epoch 6/10 loss -0.24383 val_loss -0.24383 \n",
            "Epoch 6/10 loss -2.52319 val_loss -2.52319 \n",
            "Epoch 6/10 loss 0.83634 val_loss 0.83634 \n",
            "Epoch 6/10 loss -0.79719 val_loss -0.79719 \n",
            "Epoch 6/10 loss -3.08595 val_loss -3.08595 \n",
            "Epoch 6/10 loss -1.39548 val_loss -1.39548 \n",
            "Epoch 6/10 loss 6.59514 val_loss 6.59514 \n",
            "Epoch 6/10 loss 1.42363 val_loss 1.42363 \n",
            "Epoch 6/10 loss -1.37686 val_loss -1.37686 \n",
            "Epoch 6/10 loss -4.13663 val_loss -4.13663 \n",
            "Epoch 6/10 loss 2.11500 val_loss 2.11500 \n",
            "Epoch 6/10 loss 0.24623 val_loss 0.24623 \n",
            "Epoch 6/10 loss 0.77796 val_loss 0.77796 \n",
            "Epoch 6/10 loss 2.90881 val_loss 2.90881 \n",
            "Epoch 6/10 loss -8.28508 val_loss -8.28508 \n",
            "Epoch 6/10 loss 0.89129 val_loss 0.89129 \n",
            "Epoch 6/10 loss 0.24007 val_loss 0.24007 \n",
            "Epoch 6/10 loss 0.89571 val_loss 0.89571 \n",
            "Epoch 6/10 loss -7.04357 val_loss -7.04357 \n",
            "Epoch 6/10 loss -0.43664 val_loss -0.43664 \n",
            "Epoch 6/10 loss 1.61137 val_loss 1.61137 \n",
            "Epoch 6/10 loss 2.32199 val_loss 2.32199 \n",
            "Epoch 6/10 loss 1.58820 val_loss 1.58820 \n",
            "Epoch 6/10 loss -5.08441 val_loss -5.08441 \n",
            "Epoch 6/10 loss -4.41283 val_loss -4.41283 \n",
            "Epoch 6/10 loss -0.56331 val_loss -0.56331 \n",
            "Epoch 6/10 loss 0.21127 val_loss 0.21127 \n",
            "Epoch 6/10 loss -5.09267 val_loss -5.09267 \n",
            "Epoch 6/10 loss -0.62117 val_loss -0.62117 \n",
            "Epoch 6/10 loss 1.84311 val_loss 1.84311 \n",
            "Epoch 6/10 loss 3.78550 val_loss 3.78550 \n",
            "Epoch 6/10 loss 1.06841 val_loss 1.06841 \n",
            "Epoch 6/10 loss -8.38622 val_loss -8.38622 \n",
            "Epoch 6/10 loss -0.81156 val_loss -0.81156 \n",
            "Epoch 6/10 loss 0.13021 val_loss 0.13021 \n",
            "Epoch 6/10 loss 3.18352 val_loss 3.18352 \n",
            "Epoch 6/10 loss 0.00876 val_loss 0.00876 \n",
            "Epoch 6/10 loss 0.12081 val_loss 0.12081 \n",
            "Epoch 6/10 loss 0.11976 val_loss 0.11976 \n",
            "Epoch 6/10 loss -0.91833 val_loss -0.91833 \n",
            "Epoch 6/10 loss 3.27923 val_loss 3.27923 \n",
            "Epoch 6/10 loss -0.92598 val_loss -0.92598 \n",
            "Epoch 6/10 loss 4.29577 val_loss 4.29577 \n",
            "Epoch 6/10 loss -0.88286 val_loss -0.88286 \n",
            "Epoch 6/10 loss 2.10524 val_loss 2.10524 \n",
            "Epoch 6/10 loss -0.81290 val_loss -0.81290 \n",
            "Epoch 6/10 loss 0.14490 val_loss 0.14490 \n",
            "Epoch 6/10 loss 1.98144 val_loss 1.98144 \n",
            "Epoch 6/10 loss -1.61460 val_loss -1.61460 \n",
            "Epoch 6/10 loss -1.60285 val_loss -1.60285 \n",
            "Epoch 6/10 loss -2.54529 val_loss -2.54529 \n",
            "Epoch 6/10 loss 2.03827 val_loss 2.03827 \n",
            "Epoch 6/10 loss -2.41875 val_loss -2.41875 \n",
            "Epoch 6/10 loss 0.13181 val_loss 0.13181 \n",
            "Epoch 6/10 loss 3.14608 val_loss 3.14608 \n",
            "Epoch 6/10 loss 4.47581 val_loss 4.47581 \n",
            "Epoch 6/10 loss 3.04931 val_loss 3.04931 \n",
            "Epoch 6/10 loss 0.14607 val_loss 0.14607 \n",
            "Epoch 6/10 loss 1.92773 val_loss 1.92773 \n",
            "Epoch 6/10 loss 0.17322 val_loss 0.17322 \n",
            "Epoch 6/10 loss 2.55273 val_loss 2.55273 \n",
            "Epoch 6/10 loss -1.23156 val_loss -1.23156 \n",
            "Epoch 6/10 loss 4.84243 val_loss 4.84243 \n",
            "Epoch 6/10 loss 0.24730 val_loss 0.24730 \n",
            "Epoch 6/10 loss 1.45491 val_loss 1.45491 \n",
            "Epoch 6/10 loss 0.83347 val_loss 0.83347 \n",
            "Epoch 6/10 loss -3.74104 val_loss -3.74104 \n",
            "Epoch 6/10 loss 0.79375 val_loss 0.79375 \n",
            "Epoch 6/10 loss -0.06295 val_loss -0.06295 \n",
            "Epoch 6/10 loss -7.32778 val_loss -7.32778 \n",
            "Epoch 6/10 loss 0.77699 val_loss 0.77699 \n",
            "Epoch 6/10 loss 0.77872 val_loss 0.77872 \n",
            "Epoch 6/10 loss 8.33755 val_loss 8.33755 \n",
            "Epoch 6/10 loss -0.26290 val_loss -0.26290 \n",
            "Epoch 6/10 loss 0.33098 val_loss 0.33098 \n",
            "Epoch 6/10 loss -0.68730 val_loss -0.68730 \n",
            "Epoch 6/10 loss 0.28426 val_loss 0.28426 \n",
            "Epoch 6/10 loss 1.47670 val_loss 1.47670 \n",
            "Epoch 6/10 loss -5.16354 val_loss -5.16354 \n",
            "Epoch 6/10 loss 3.03520 val_loss 3.03520 \n",
            "Epoch 6/10 loss 1.55323 val_loss 1.55323 \n",
            "Epoch 6/10 loss 0.21138 val_loss 0.21138 \n",
            "Epoch 6/10 loss -1.96947 val_loss -1.96947 \n",
            "Epoch 6/10 loss 0.19649 val_loss 0.19649 \n",
            "Epoch 6/10 loss -3.01788 val_loss -3.01788 \n",
            "Epoch 6/10 loss -2.13478 val_loss -2.13478 \n",
            "Epoch 6/10 loss -3.60600 val_loss -3.60600 \n",
            "Epoch 6/10 loss 1.18224 val_loss 1.18224 \n",
            "Epoch 6/10 loss 3.39911 val_loss 3.39911 \n",
            "Epoch 6/10 loss -11.19791 val_loss -11.19791 \n",
            "Epoch 6/10 loss -2.13355 val_loss -2.13355 \n",
            "Epoch 6/10 loss 1.32586 val_loss 1.32586 \n",
            "Epoch 6/10 loss -1.21163 val_loss -1.21163 \n",
            "Epoch 6/10 loss -2.60428 val_loss -2.60428 \n",
            "Epoch 6/10 loss 2.94743 val_loss 2.94743 \n",
            "Epoch 6/10 loss 2.91514 val_loss 2.91514 \n",
            "Epoch 6/10 loss 1.47860 val_loss 1.47860 \n",
            "Epoch 6/10 loss 0.05730 val_loss 0.05730 \n",
            "Epoch 6/10 loss -4.18098 val_loss -4.18098 \n",
            "Epoch 6/10 loss 5.87325 val_loss 5.87325 \n",
            "Epoch 6/10 loss 2.94039 val_loss 2.94039 \n",
            "Epoch 6/10 loss 1.46773 val_loss 1.46773 \n",
            "Epoch 6/10 loss -1.98132 val_loss -1.98132 \n",
            "Epoch 6/10 loss 0.06893 val_loss 0.06893 \n",
            "Epoch 6/10 loss -2.65404 val_loss -2.65404 \n",
            "Epoch 6/10 loss 4.73753 val_loss 4.73753 \n",
            "Epoch 6/10 loss 3.68297 val_loss 3.68297 \n",
            "Epoch 6/10 loss -2.10842 val_loss -2.10842 \n",
            "Epoch 6/10 loss -3.15170 val_loss -3.15170 \n",
            "Epoch 6/10 loss -2.23930 val_loss -2.23930 \n",
            "Epoch 6/10 loss 0.10764 val_loss 0.10764 \n",
            "Epoch 6/10 loss -2.10607 val_loss -2.10607 \n",
            "Epoch 6/10 loss -2.09462 val_loss -2.09462 \n",
            "Epoch 6/10 loss 0.09702 val_loss 0.09702 \n",
            "Epoch 6/10 loss 2.45492 val_loss 2.45492 \n",
            "Epoch 6/10 loss 0.24157 val_loss 0.24157 \n",
            "Epoch 6/10 loss 1.10496 val_loss 1.10496 \n",
            "Epoch 6/10 loss 1.27467 val_loss 1.27467 \n",
            "Epoch 6/10 loss -10.08140 val_loss -10.08140 \n",
            "Epoch 6/10 loss 3.75352 val_loss 3.75352 \n",
            "Epoch 6/10 loss 5.00118 val_loss 5.00118 \n",
            "Epoch 6/10 loss 0.49409 val_loss 0.49409 \n",
            "Epoch 6/10 loss 2.44721 val_loss 2.44721 \n",
            "Epoch 6/10 loss -0.73637 val_loss -0.73637 \n",
            "Epoch 6/10 loss 0.10631 val_loss 0.10631 \n",
            "Epoch 6/10 loss -1.98518 val_loss -1.98518 \n",
            "Epoch 6/10 loss 0.11884 val_loss 0.11884 \n",
            "Epoch 6/10 loss -6.82537 val_loss -6.82537 \n",
            "Epoch 6/10 loss 2.15298 val_loss 2.15298 \n",
            "Epoch 6/10 loss -0.85760 val_loss -0.85760 \n",
            "Epoch 6/10 loss 1.10919 val_loss 1.10919 \n",
            "Epoch 6/10 loss 0.13720 val_loss 0.13720 \n",
            "Epoch 6/10 loss -1.71533 val_loss -1.71533 \n",
            "Epoch 6/10 loss -6.84754 val_loss -6.84754 \n",
            "Epoch 6/10 loss 0.15141 val_loss 0.15141 \n",
            "Epoch 6/10 loss 2.83265 val_loss 2.83265 \n",
            "Epoch 6/10 loss 2.72162 val_loss 2.72162 \n",
            "Epoch 6/10 loss -4.13422 val_loss -4.13422 \n",
            "Epoch 6/10 loss 0.20483 val_loss 0.20483 \n",
            "Epoch 6/10 loss -4.77600 val_loss -4.77600 \n",
            "Epoch 7/10 loss -0.46484 val_loss -0.46484 \n",
            "Epoch 7/10 loss 0.92015 val_loss 0.92015 \n",
            "Epoch 7/10 loss -2.69324 val_loss -2.69324 \n",
            "Epoch 7/10 loss -8.26086 val_loss -8.26086 \n",
            "Epoch 7/10 loss -8.30785 val_loss -8.30785 \n",
            "Epoch 7/10 loss 1.67867 val_loss 1.67867 \n",
            "Epoch 7/10 loss 0.95258 val_loss 0.95258 \n",
            "Epoch 7/10 loss -2.83918 val_loss -2.83918 \n",
            "Epoch 7/10 loss -7.84632 val_loss -7.84632 \n",
            "Epoch 7/10 loss 2.78080 val_loss 2.78080 \n",
            "Epoch 7/10 loss -6.20922 val_loss -6.20922 \n",
            "Epoch 7/10 loss 3.00405 val_loss 3.00405 \n",
            "Epoch 7/10 loss 2.08620 val_loss 2.08620 \n",
            "Epoch 7/10 loss 1.10982 val_loss 1.10982 \n",
            "Epoch 7/10 loss -0.83560 val_loss -0.83560 \n",
            "Epoch 7/10 loss 3.06222 val_loss 3.06222 \n",
            "Epoch 7/10 loss -2.45187 val_loss -2.45187 \n",
            "Epoch 7/10 loss -1.76920 val_loss -1.76920 \n",
            "Epoch 7/10 loss 1.98423 val_loss 1.98423 \n",
            "Epoch 7/10 loss 1.76035 val_loss 1.76035 \n",
            "Epoch 7/10 loss 3.74598 val_loss 3.74598 \n",
            "Epoch 7/10 loss -1.66269 val_loss -1.66269 \n",
            "Epoch 7/10 loss -0.62678 val_loss -0.62678 \n",
            "Epoch 7/10 loss -2.13046 val_loss -2.13046 \n",
            "Epoch 7/10 loss 3.59125 val_loss 3.59125 \n",
            "Epoch 7/10 loss 0.18967 val_loss 0.18967 \n",
            "Epoch 7/10 loss 0.18541 val_loss 0.18541 \n",
            "Epoch 7/10 loss -0.62618 val_loss -0.62618 \n",
            "Epoch 7/10 loss -0.90617 val_loss -0.90617 \n",
            "Epoch 7/10 loss 2.65035 val_loss 2.65035 \n",
            "Epoch 7/10 loss 0.97714 val_loss 0.97714 \n",
            "Epoch 7/10 loss -5.08503 val_loss -5.08503 \n",
            "Epoch 7/10 loss 0.95154 val_loss 0.95154 \n",
            "Epoch 7/10 loss -8.81779 val_loss -8.81779 \n",
            "Epoch 7/10 loss 2.47415 val_loss 2.47415 \n",
            "Epoch 7/10 loss 0.20310 val_loss 0.20310 \n",
            "Epoch 7/10 loss 3.15511 val_loss 3.15511 \n",
            "Epoch 7/10 loss -1.98100 val_loss -1.98100 \n",
            "Epoch 7/10 loss -0.38361 val_loss -0.38361 \n",
            "Epoch 7/10 loss 0.85983 val_loss 0.85983 \n",
            "Epoch 7/10 loss 1.39246 val_loss 1.39246 \n",
            "Epoch 7/10 loss 0.31258 val_loss 0.31258 \n",
            "Epoch 7/10 loss -0.75212 val_loss -0.75212 \n",
            "Epoch 7/10 loss 0.34295 val_loss 0.34295 \n",
            "Epoch 7/10 loss 0.34693 val_loss 0.34693 \n",
            "Epoch 7/10 loss 0.34563 val_loss 0.34563 \n",
            "Epoch 7/10 loss -0.10772 val_loss -0.10772 \n",
            "Epoch 7/10 loss -0.14359 val_loss -0.14359 \n",
            "Epoch 7/10 loss 0.81751 val_loss 0.81751 \n",
            "Epoch 7/10 loss 1.90598 val_loss 1.90598 \n",
            "Epoch 7/10 loss -1.31645 val_loss -1.31645 \n",
            "Epoch 7/10 loss -5.35686 val_loss -5.35686 \n",
            "Epoch 7/10 loss -1.48391 val_loss -1.48391 \n",
            "Epoch 7/10 loss 0.90246 val_loss 0.90246 \n",
            "Epoch 7/10 loss 1.49580 val_loss 1.49580 \n",
            "Epoch 7/10 loss -1.16046 val_loss -1.16046 \n",
            "Epoch 7/10 loss 0.34203 val_loss 0.34203 \n",
            "Epoch 7/10 loss 2.36962 val_loss 2.36962 \n",
            "Epoch 7/10 loss 0.91628 val_loss 0.91628 \n",
            "Epoch 7/10 loss 2.22642 val_loss 2.22642 \n",
            "Epoch 7/10 loss -4.77851 val_loss -4.77851 \n",
            "Epoch 7/10 loss -1.75865 val_loss -1.75865 \n",
            "Epoch 7/10 loss -1.30917 val_loss -1.30917 \n",
            "Epoch 7/10 loss -1.80150 val_loss -1.80150 \n",
            "Epoch 7/10 loss 0.28060 val_loss 0.28060 \n",
            "Epoch 7/10 loss 0.26189 val_loss 0.26189 \n",
            "Epoch 7/10 loss -0.39949 val_loss -0.39949 \n",
            "Epoch 7/10 loss -2.81453 val_loss -2.81453 \n",
            "Epoch 7/10 loss 2.43200 val_loss 2.43200 \n",
            "Epoch 7/10 loss 2.47382 val_loss 2.47382 \n",
            "Epoch 7/10 loss 2.44031 val_loss 2.44031 \n",
            "Epoch 7/10 loss 2.33866 val_loss 2.33866 \n",
            "Epoch 7/10 loss -4.23985 val_loss -4.23985 \n",
            "Epoch 7/10 loss 0.26215 val_loss 0.26215 \n",
            "Epoch 7/10 loss 1.41504 val_loss 1.41504 \n",
            "Epoch 7/10 loss 0.30140 val_loss 0.30140 \n",
            "Epoch 7/10 loss 1.29299 val_loss 1.29299 \n",
            "Epoch 7/10 loss -1.40639 val_loss -1.40639 \n",
            "Epoch 7/10 loss 1.23132 val_loss 1.23132 \n",
            "Epoch 7/10 loss 0.78394 val_loss 0.78394 \n",
            "Epoch 7/10 loss 0.77844 val_loss 0.77844 \n",
            "Epoch 7/10 loss 11.62380 val_loss 11.62380 \n",
            "Epoch 7/10 loss 0.39848 val_loss 0.39848 \n",
            "Epoch 7/10 loss -0.54762 val_loss -0.54762 \n",
            "Epoch 7/10 loss -7.57604 val_loss -7.57604 \n",
            "Epoch 7/10 loss -3.33707 val_loss -3.33707 \n",
            "Epoch 7/10 loss -0.67588 val_loss -0.67588 \n",
            "Epoch 7/10 loss -2.30105 val_loss -2.30105 \n",
            "Epoch 7/10 loss 1.19175 val_loss 1.19175 \n",
            "Epoch 7/10 loss 1.18752 val_loss 1.18752 \n",
            "Epoch 7/10 loss 2.16688 val_loss 2.16688 \n",
            "Epoch 7/10 loss -0.05208 val_loss -0.05208 \n",
            "Epoch 7/10 loss 1.21146 val_loss 1.21146 \n",
            "Epoch 7/10 loss -2.21440 val_loss -2.21440 \n",
            "Epoch 7/10 loss -0.48038 val_loss -0.48038 \n",
            "Epoch 7/10 loss 9.15651 val_loss 9.15651 \n",
            "Epoch 7/10 loss -1.64935 val_loss -1.64935 \n",
            "Epoch 7/10 loss -1.05365 val_loss -1.05365 \n",
            "Epoch 7/10 loss -2.92174 val_loss -2.92174 \n",
            "Epoch 7/10 loss 0.18282 val_loss 0.18282 \n",
            "Epoch 7/10 loss 2.83122 val_loss 2.83122 \n",
            "Epoch 7/10 loss 2.02969 val_loss 2.02969 \n",
            "Epoch 7/10 loss 0.13333 val_loss 0.13333 \n",
            "Epoch 7/10 loss -0.87766 val_loss -0.87766 \n",
            "Epoch 7/10 loss 2.14270 val_loss 2.14270 \n",
            "Epoch 7/10 loss 4.34463 val_loss 4.34463 \n",
            "Epoch 7/10 loss 0.11837 val_loss 0.11837 \n",
            "Epoch 7/10 loss 1.13769 val_loss 1.13769 \n",
            "Epoch 7/10 loss 1.11845 val_loss 1.11845 \n",
            "Epoch 7/10 loss 0.13717 val_loss 0.13717 \n",
            "Epoch 7/10 loss 1.94117 val_loss 1.94117 \n",
            "Epoch 7/10 loss -2.52241 val_loss -2.52241 \n",
            "Epoch 7/10 loss 0.15402 val_loss 0.15402 \n",
            "Epoch 7/10 loss -13.88125 val_loss -13.88125 \n",
            "Epoch 7/10 loss -4.48396 val_loss -4.48396 \n",
            "Epoch 7/10 loss -5.75809 val_loss -5.75809 \n",
            "Epoch 7/10 loss -3.58650 val_loss -3.58650 \n",
            "Epoch 7/10 loss 1.19144 val_loss 1.19144 \n",
            "Epoch 7/10 loss 4.61727 val_loss 4.61727 \n",
            "Epoch 7/10 loss -12.01391 val_loss -12.01391 \n",
            "Epoch 7/10 loss 4.79898 val_loss 4.79898 \n",
            "Epoch 7/10 loss 1.26841 val_loss 1.26841 \n",
            "Epoch 7/10 loss -8.26322 val_loss -8.26322 \n",
            "Epoch 7/10 loss 7.39820 val_loss 7.39820 \n",
            "Epoch 7/10 loss 1.26079 val_loss 1.26079 \n",
            "Epoch 7/10 loss -5.95002 val_loss -5.95002 \n",
            "Epoch 7/10 loss 2.34939 val_loss 2.34939 \n",
            "Epoch 7/10 loss 0.10756 val_loss 0.10756 \n",
            "Epoch 7/10 loss -2.00048 val_loss -2.00048 \n",
            "Epoch 7/10 loss 4.32201 val_loss 4.32201 \n",
            "Epoch 7/10 loss 0.12421 val_loss 0.12421 \n",
            "Epoch 7/10 loss 2.01203 val_loss 2.01203 \n",
            "Epoch 7/10 loss -2.61647 val_loss -2.61647 \n",
            "Epoch 7/10 loss 0.15082 val_loss 0.15082 \n",
            "Epoch 7/10 loss -1.64359 val_loss -1.64359 \n",
            "Epoch 7/10 loss -0.91719 val_loss -0.91719 \n",
            "Epoch 7/10 loss 1.05108 val_loss 1.05108 \n",
            "Epoch 7/10 loss 0.15994 val_loss 0.15994 \n",
            "Epoch 7/10 loss -1.55546 val_loss -1.55546 \n",
            "Epoch 7/10 loss -0.70655 val_loss -0.70655 \n",
            "Epoch 7/10 loss 4.73213 val_loss 4.73213 \n",
            "Epoch 7/10 loss -4.58634 val_loss -4.58634 \n",
            "Epoch 7/10 loss -0.76906 val_loss -0.76906 \n",
            "Epoch 7/10 loss -3.77623 val_loss -3.77623 \n",
            "Epoch 7/10 loss -1.83036 val_loss -1.83036 \n",
            "Epoch 7/10 loss 3.40994 val_loss 3.40994 \n",
            "Epoch 7/10 loss -0.95477 val_loss -0.95477 \n",
            "Epoch 7/10 loss 4.45144 val_loss 4.45144 \n",
            "Epoch 7/10 loss -3.05114 val_loss -3.05114 \n",
            "Epoch 7/10 loss -0.96444 val_loss -0.96444 \n",
            "Epoch 8/10 loss 3.41768 val_loss 3.41768 \n",
            "Epoch 8/10 loss 1.20412 val_loss 1.20412 \n",
            "Epoch 8/10 loss -10.38634 val_loss -10.38634 \n",
            "Epoch 8/10 loss 3.45657 val_loss 3.45657 \n",
            "Epoch 8/10 loss 3.45713 val_loss 3.45713 \n",
            "Epoch 8/10 loss 0.10840 val_loss 0.10840 \n",
            "Epoch 8/10 loss 3.30971 val_loss 3.30971 \n",
            "Epoch 8/10 loss -0.74543 val_loss -0.74543 \n",
            "Epoch 8/10 loss -0.78785 val_loss -0.78785 \n",
            "Epoch 8/10 loss 1.03596 val_loss 1.03596 \n",
            "Epoch 8/10 loss -2.97493 val_loss -2.97493 \n",
            "Epoch 8/10 loss 3.26815 val_loss 3.26815 \n",
            "Epoch 8/10 loss 1.59536 val_loss 1.59536 \n",
            "Epoch 8/10 loss -0.91436 val_loss -0.91436 \n",
            "Epoch 8/10 loss -0.24223 val_loss -0.24223 \n",
            "Epoch 8/10 loss -7.71925 val_loss -7.71925 \n",
            "Epoch 8/10 loss -0.30440 val_loss -0.30440 \n",
            "Epoch 8/10 loss 2.16075 val_loss 2.16075 \n",
            "Epoch 8/10 loss -4.12383 val_loss -4.12383 \n",
            "Epoch 8/10 loss -2.87718 val_loss -2.87718 \n",
            "Epoch 8/10 loss 0.41618 val_loss 0.41618 \n",
            "Epoch 8/10 loss -3.63392 val_loss -3.63392 \n",
            "Epoch 8/10 loss 0.18776 val_loss 0.18776 \n",
            "Epoch 8/10 loss -2.83235 val_loss -2.83235 \n",
            "Epoch 8/10 loss -2.68050 val_loss -2.68050 \n",
            "Epoch 8/10 loss 1.15282 val_loss 1.15282 \n",
            "Epoch 8/10 loss -0.01560 val_loss -0.01560 \n",
            "Epoch 8/10 loss 0.52204 val_loss 0.52204 \n",
            "Epoch 8/10 loss 0.25526 val_loss 0.25526 \n",
            "Epoch 8/10 loss 0.93946 val_loss 0.93946 \n",
            "Epoch 8/10 loss 1.16807 val_loss 1.16807 \n",
            "Epoch 8/10 loss 0.10032 val_loss 0.10032 \n",
            "Epoch 8/10 loss -4.66106 val_loss -4.66106 \n",
            "Epoch 8/10 loss -2.63307 val_loss -2.63307 \n",
            "Epoch 8/10 loss -7.98748 val_loss -7.98748 \n",
            "Epoch 8/10 loss 0.41758 val_loss 0.41758 \n",
            "Epoch 8/10 loss 1.50808 val_loss 1.50808 \n",
            "Epoch 8/10 loss 1.93300 val_loss 1.93300 \n",
            "Epoch 8/10 loss 0.39160 val_loss 0.39160 \n",
            "Epoch 8/10 loss 0.39960 val_loss 0.39960 \n",
            "Epoch 8/10 loss 1.45606 val_loss 1.45606 \n",
            "Epoch 8/10 loss -4.75335 val_loss -4.75335 \n",
            "Epoch 8/10 loss -3.38943 val_loss -3.38943 \n",
            "Epoch 8/10 loss -0.92383 val_loss -0.92383 \n",
            "Epoch 8/10 loss 1.14922 val_loss 1.14922 \n",
            "Epoch 8/10 loss -3.75430 val_loss -3.75430 \n",
            "Epoch 8/10 loss -5.17874 val_loss -5.17874 \n",
            "Epoch 8/10 loss 0.30956 val_loss 0.30956 \n",
            "Epoch 8/10 loss -9.61209 val_loss -9.61209 \n",
            "Epoch 8/10 loss -7.94211 val_loss -7.94211 \n",
            "Epoch 8/10 loss -3.20037 val_loss -3.20037 \n",
            "Epoch 8/10 loss -1.42762 val_loss -1.42762 \n",
            "Epoch 8/10 loss -3.45978 val_loss -3.45978 \n",
            "Epoch 8/10 loss 2.23106 val_loss 2.23106 \n",
            "Epoch 8/10 loss 1.24265 val_loss 1.24265 \n",
            "Epoch 8/10 loss 2.50942 val_loss 2.50942 \n",
            "Epoch 8/10 loss 6.60823 val_loss 6.60823 \n",
            "Epoch 8/10 loss 0.07887 val_loss 0.07887 \n",
            "Epoch 8/10 loss 2.55338 val_loss 2.55338 \n",
            "Epoch 8/10 loss 11.12890 val_loss 11.12890 \n",
            "Epoch 8/10 loss 3.78462 val_loss 3.78462 \n",
            "Epoch 8/10 loss -1.10933 val_loss -1.10933 \n",
            "Epoch 8/10 loss -2.26670 val_loss -2.26670 \n",
            "Epoch 8/10 loss 4.84862 val_loss 4.84862 \n",
            "Epoch 8/10 loss -0.44413 val_loss -0.44413 \n",
            "Epoch 8/10 loss -0.97863 val_loss -0.97863 \n",
            "Epoch 8/10 loss 3.23948 val_loss 3.23948 \n",
            "Epoch 8/10 loss 0.06761 val_loss 0.06761 \n",
            "Epoch 8/10 loss 1.15763 val_loss 1.15763 \n",
            "Epoch 8/10 loss 1.03804 val_loss 1.03804 \n",
            "Epoch 8/10 loss 3.57004 val_loss 3.57004 \n",
            "Epoch 8/10 loss 0.19005 val_loss 0.19005 \n",
            "Epoch 8/10 loss 0.21177 val_loss 0.21177 \n",
            "Epoch 8/10 loss -2.47672 val_loss -2.47672 \n",
            "Epoch 8/10 loss 2.84373 val_loss 2.84373 \n",
            "Epoch 8/10 loss -2.66932 val_loss -2.66932 \n",
            "Epoch 8/10 loss 5.00149 val_loss 5.00149 \n",
            "Epoch 8/10 loss -12.96312 val_loss -12.96312 \n",
            "Epoch 8/10 loss 2.01750 val_loss 2.01750 \n",
            "Epoch 8/10 loss 1.10763 val_loss 1.10763 \n",
            "Epoch 8/10 loss 1.12852 val_loss 1.12852 \n",
            "Epoch 8/10 loss 2.15769 val_loss 2.15769 \n",
            "Epoch 8/10 loss 0.13049 val_loss 0.13049 \n",
            "Epoch 8/10 loss -0.86475 val_loss -0.86475 \n",
            "Epoch 8/10 loss -7.36694 val_loss -7.36694 \n",
            "Epoch 8/10 loss 4.31161 val_loss 4.31161 \n",
            "Epoch 8/10 loss 2.17279 val_loss 2.17279 \n",
            "Epoch 8/10 loss -2.82005 val_loss -2.82005 \n",
            "Epoch 8/10 loss 2.10092 val_loss 2.10092 \n",
            "Epoch 8/10 loss 0.13549 val_loss 0.13549 \n",
            "Epoch 8/10 loss 3.94243 val_loss 3.94243 \n",
            "Epoch 8/10 loss -3.41654 val_loss -3.41654 \n",
            "Epoch 8/10 loss -0.73701 val_loss -0.73701 \n",
            "Epoch 8/10 loss -4.03832 val_loss -4.03832 \n",
            "Epoch 8/10 loss 3.83347 val_loss 3.83347 \n",
            "Epoch 8/10 loss 0.15555 val_loss 0.15555 \n",
            "Epoch 8/10 loss -4.02263 val_loss -4.02263 \n",
            "Epoch 8/10 loss 0.16509 val_loss 0.16509 \n",
            "Epoch 8/10 loss -0.68834 val_loss -0.68834 \n",
            "Epoch 8/10 loss -0.70411 val_loss -0.70411 \n",
            "Epoch 8/10 loss 3.72595 val_loss 3.72595 \n",
            "Epoch 8/10 loss -4.90718 val_loss -4.90718 \n",
            "Epoch 8/10 loss -1.57371 val_loss -1.57371 \n",
            "Epoch 8/10 loss 1.93675 val_loss 1.93675 \n",
            "Epoch 8/10 loss -3.40854 val_loss -3.40854 \n",
            "Epoch 8/10 loss 1.08248 val_loss 1.08248 \n",
            "Epoch 8/10 loss 2.08644 val_loss 2.08644 \n",
            "Epoch 8/10 loss 12.23670 val_loss 12.23670 \n",
            "Epoch 8/10 loss 2.05017 val_loss 2.05017 \n",
            "Epoch 8/10 loss 1.88849 val_loss 1.88849 \n",
            "Epoch 8/10 loss -0.66957 val_loss -0.66957 \n",
            "Epoch 8/10 loss -2.20251 val_loss -2.20251 \n",
            "Epoch 8/10 loss 0.18380 val_loss 0.18380 \n",
            "Epoch 8/10 loss 0.18516 val_loss 0.18516 \n",
            "Epoch 8/10 loss 0.98535 val_loss 0.98535 \n",
            "Epoch 8/10 loss 0.89248 val_loss 0.89248 \n",
            "Epoch 8/10 loss 0.97616 val_loss 0.97616 \n",
            "Epoch 8/10 loss 1.63843 val_loss 1.63843 \n",
            "Epoch 8/10 loss -5.35789 val_loss -5.35789 \n",
            "Epoch 8/10 loss 1.67951 val_loss 1.67951 \n",
            "Epoch 8/10 loss 1.65405 val_loss 1.65405 \n",
            "Epoch 8/10 loss -8.96740 val_loss -8.96740 \n",
            "Epoch 8/10 loss 0.22668 val_loss 0.22668 \n",
            "Epoch 8/10 loss -6.75166 val_loss -6.75166 \n",
            "Epoch 8/10 loss 1.56133 val_loss 1.56133 \n",
            "Epoch 8/10 loss -5.01468 val_loss -5.01468 \n",
            "Epoch 8/10 loss -1.00543 val_loss -1.00543 \n",
            "Epoch 8/10 loss 0.89545 val_loss 0.89545 \n",
            "Epoch 8/10 loss -1.80520 val_loss -1.80520 \n",
            "Epoch 8/10 loss 0.83849 val_loss 0.83849 \n",
            "Epoch 8/10 loss -0.58485 val_loss -0.58485 \n",
            "Epoch 8/10 loss 1.00379 val_loss 1.00379 \n",
            "Epoch 8/10 loss -3.31628 val_loss -3.31628 \n",
            "Epoch 8/10 loss -2.72302 val_loss -2.72302 \n",
            "Epoch 8/10 loss 2.26078 val_loss 2.26078 \n",
            "Epoch 8/10 loss -4.12646 val_loss -4.12646 \n",
            "Epoch 8/10 loss 7.83520 val_loss 7.83520 \n",
            "Epoch 8/10 loss 2.63642 val_loss 2.63642 \n",
            "Epoch 8/10 loss 3.99870 val_loss 3.99870 \n",
            "Epoch 8/10 loss -2.53439 val_loss -2.53439 \n",
            "Epoch 8/10 loss 0.06879 val_loss 0.06879 \n",
            "Epoch 8/10 loss 1.98669 val_loss 1.98669 \n",
            "Epoch 8/10 loss 5.52956 val_loss 5.52956 \n",
            "Epoch 8/10 loss -2.63063 val_loss -2.63063 \n",
            "Epoch 8/10 loss -11.82865 val_loss -11.82865 \n",
            "Epoch 8/10 loss -1.33645 val_loss -1.33645 \n",
            "Epoch 8/10 loss -1.39022 val_loss -1.39022 \n",
            "Epoch 8/10 loss -2.65503 val_loss -2.65503 \n",
            "Epoch 8/10 loss -1.51797 val_loss -1.51797 \n",
            "Epoch 8/10 loss 3.21678 val_loss 3.21678 \n",
            "Epoch 9/10 loss -0.94777 val_loss -0.94777 \n",
            "Epoch 9/10 loss 3.25327 val_loss 3.25327 \n",
            "Epoch 9/10 loss -2.01082 val_loss -2.01082 \n",
            "Epoch 9/10 loss 0.04389 val_loss 0.04389 \n",
            "Epoch 9/10 loss -1.47756 val_loss -1.47756 \n",
            "Epoch 9/10 loss -2.97456 val_loss -2.97456 \n",
            "Epoch 9/10 loss -2.99894 val_loss -2.99894 \n",
            "Epoch 9/10 loss 3.15613 val_loss 3.15613 \n",
            "Epoch 9/10 loss -4.65332 val_loss -4.65332 \n",
            "Epoch 9/10 loss 1.64654 val_loss 1.64654 \n",
            "Epoch 9/10 loss 1.67261 val_loss 1.67261 \n",
            "Epoch 9/10 loss 0.93169 val_loss 0.93169 \n",
            "Epoch 9/10 loss 3.40538 val_loss 3.40538 \n",
            "Epoch 9/10 loss 3.42622 val_loss 3.42622 \n",
            "Epoch 9/10 loss 3.40368 val_loss 3.40368 \n",
            "Epoch 9/10 loss -8.25947 val_loss -8.25947 \n",
            "Epoch 9/10 loss 0.78243 val_loss 0.78243 \n",
            "Epoch 9/10 loss 0.03841 val_loss 0.03841 \n",
            "Epoch 9/10 loss 4.85436 val_loss 4.85436 \n",
            "Epoch 9/10 loss -3.28430 val_loss -3.28430 \n",
            "Epoch 9/10 loss 4.55501 val_loss 4.55501 \n",
            "Epoch 9/10 loss 2.90966 val_loss 2.90966 \n",
            "Epoch 9/10 loss -8.19473 val_loss -8.19473 \n",
            "Epoch 9/10 loss -1.21945 val_loss -1.21945 \n",
            "Epoch 9/10 loss -9.19440 val_loss -9.19440 \n",
            "Epoch 9/10 loss -4.95900 val_loss -4.95900 \n",
            "Epoch 9/10 loss 2.67296 val_loss 2.67296 \n",
            "Epoch 9/10 loss 9.33743 val_loss 9.33743 \n",
            "Epoch 9/10 loss -1.24181 val_loss -1.24181 \n",
            "Epoch 9/10 loss -5.20836 val_loss -5.20836 \n",
            "Epoch 9/10 loss -11.17974 val_loss -11.17974 \n",
            "Epoch 9/10 loss 0.05471 val_loss 0.05471 \n",
            "Epoch 9/10 loss 0.04830 val_loss 0.04830 \n",
            "Epoch 9/10 loss 4.72393 val_loss 4.72393 \n",
            "Epoch 9/10 loss 3.18411 val_loss 3.18411 \n",
            "Epoch 9/10 loss 0.04172 val_loss 0.04172 \n",
            "Epoch 9/10 loss 0.04191 val_loss 0.04191 \n",
            "Epoch 9/10 loss 1.61654 val_loss 1.61654 \n",
            "Epoch 9/10 loss 4.73438 val_loss 4.73438 \n",
            "Epoch 9/10 loss -2.99852 val_loss -2.99852 \n",
            "Epoch 9/10 loss 0.04666 val_loss 0.04666 \n",
            "Epoch 9/10 loss 3.04094 val_loss 3.04094 \n",
            "Epoch 9/10 loss -14.24490 val_loss -14.24490 \n",
            "Epoch 9/10 loss -5.73566 val_loss -5.73566 \n",
            "Epoch 9/10 loss 1.52659 val_loss 1.52659 \n",
            "Epoch 9/10 loss -1.44237 val_loss -1.44237 \n",
            "Epoch 9/10 loss 0.04694 val_loss 0.04694 \n",
            "Epoch 9/10 loss 6.69616 val_loss 6.69616 \n",
            "Epoch 9/10 loss 1.62877 val_loss 1.62877 \n",
            "Epoch 9/10 loss 5.00890 val_loss 5.00890 \n",
            "Epoch 9/10 loss -2.60650 val_loss -2.60650 \n",
            "Epoch 9/10 loss -4.97605 val_loss -4.97605 \n",
            "Epoch 9/10 loss 1.73619 val_loss 1.73619 \n",
            "Epoch 9/10 loss -8.10585 val_loss -8.10585 \n",
            "Epoch 9/10 loss -1.71112 val_loss -1.71112 \n",
            "Epoch 9/10 loss -1.74028 val_loss -1.74028 \n",
            "Epoch 9/10 loss 5.69806 val_loss 5.69806 \n",
            "Epoch 9/10 loss -5.48507 val_loss -5.48507 \n",
            "Epoch 9/10 loss -1.87594 val_loss -1.87594 \n",
            "Epoch 9/10 loss -5.87443 val_loss -5.87443 \n",
            "Epoch 9/10 loss 8.24557 val_loss 8.24557 \n",
            "Epoch 9/10 loss -8.82467 val_loss -8.82467 \n",
            "Epoch 9/10 loss -2.13113 val_loss -2.13113 \n",
            "Epoch 9/10 loss -2.07788 val_loss -2.07788 \n",
            "Epoch 9/10 loss -4.28722 val_loss -4.28722 \n",
            "Epoch 9/10 loss 0.01079 val_loss 0.01079 \n",
            "Epoch 9/10 loss -1.08318 val_loss -1.08318 \n",
            "Epoch 9/10 loss -2.30823 val_loss -2.30823 \n",
            "Epoch 9/10 loss 0.00916 val_loss 0.00916 \n",
            "Epoch 9/10 loss 1.95655 val_loss 1.95655 \n",
            "Epoch 9/10 loss 7.21130 val_loss 7.21130 \n",
            "Epoch 9/10 loss 9.59483 val_loss 9.59483 \n",
            "Epoch 9/10 loss 13.86611 val_loss 13.86611 \n",
            "Epoch 9/10 loss -15.40032 val_loss -15.40032 \n",
            "Epoch 9/10 loss -4.14384 val_loss -4.14384 \n",
            "Epoch 9/10 loss 6.65159 val_loss 6.65159 \n",
            "Epoch 9/10 loss -0.38487 val_loss -0.38487 \n",
            "Epoch 9/10 loss 6.28088 val_loss 6.28088 \n",
            "Epoch 9/10 loss -1.73260 val_loss -1.73260 \n",
            "Epoch 9/10 loss -9.07664 val_loss -9.07664 \n",
            "Epoch 9/10 loss -1.82811 val_loss -1.82811 \n",
            "Epoch 9/10 loss 0.02626 val_loss 0.02626 \n",
            "Epoch 9/10 loss -15.81440 val_loss -15.81440 \n",
            "Epoch 9/10 loss 0.02653 val_loss 0.02653 \n",
            "Epoch 9/10 loss -14.17712 val_loss -14.17712 \n",
            "Epoch 9/10 loss -3.76729 val_loss -3.76729 \n",
            "Epoch 9/10 loss -1.96076 val_loss -1.96076 \n",
            "Epoch 9/10 loss -2.06146 val_loss -2.06146 \n",
            "Epoch 9/10 loss 0.01304 val_loss 0.01304 \n",
            "Epoch 9/10 loss 2.26662 val_loss 2.26662 \n",
            "Epoch 9/10 loss 9.28329 val_loss 9.28329 \n",
            "Epoch 9/10 loss -2.31712 val_loss -2.31712 \n",
            "Epoch 9/10 loss 9.36180 val_loss 9.36180 \n",
            "Epoch 9/10 loss -2.29184 val_loss -2.29184 \n",
            "Epoch 9/10 loss -4.56106 val_loss -4.56106 \n",
            "Epoch 9/10 loss -3.46521 val_loss -3.46521 \n",
            "Epoch 9/10 loss 2.31230 val_loss 2.31230 \n",
            "Epoch 9/10 loss -2.28734 val_loss -2.28734 \n",
            "Epoch 9/10 loss 9.23183 val_loss 9.23183 \n",
            "Epoch 9/10 loss 2.27544 val_loss 2.27544 \n",
            "Epoch 9/10 loss 0.05257 val_loss 0.05257 \n",
            "Epoch 9/10 loss 0.01388 val_loss 0.01388 \n",
            "Epoch 9/10 loss -2.04799 val_loss -2.04799 \n",
            "Epoch 9/10 loss -7.04243 val_loss -7.04243 \n",
            "Epoch 9/10 loss 1.96329 val_loss 1.96329 \n",
            "Epoch 9/10 loss -2.81989 val_loss -2.81989 \n",
            "Epoch 9/10 loss -0.74928 val_loss -0.74928 \n",
            "Epoch 9/10 loss 6.95639 val_loss 6.95639 \n",
            "Epoch 9/10 loss 3.21538 val_loss 3.21538 \n",
            "Epoch 9/10 loss -4.51167 val_loss -4.51167 \n",
            "Epoch 9/10 loss 2.69751 val_loss 2.69751 \n",
            "Epoch 9/10 loss 3.61861 val_loss 3.61861 \n",
            "Epoch 9/10 loss 3.13877 val_loss 3.13877 \n",
            "Epoch 9/10 loss 2.60771 val_loss 2.60771 \n",
            "Epoch 9/10 loss 0.13149 val_loss 0.13149 \n",
            "Epoch 9/10 loss 0.77852 val_loss 0.77852 \n",
            "Epoch 9/10 loss 0.04097 val_loss 0.04097 \n",
            "Epoch 9/10 loss 0.70471 val_loss 0.70471 \n",
            "Epoch 9/10 loss -3.46434 val_loss -3.46434 \n",
            "Epoch 9/10 loss 4.91728 val_loss 4.91728 \n",
            "Epoch 9/10 loss 4.14546 val_loss 4.14546 \n",
            "Epoch 9/10 loss 0.90590 val_loss 0.90590 \n",
            "Epoch 9/10 loss 1.24469 val_loss 1.24469 \n",
            "Epoch 9/10 loss 1.58591 val_loss 1.58591 \n",
            "Epoch 9/10 loss 0.98694 val_loss 0.98694 \n",
            "Epoch 9/10 loss 0.49639 val_loss 0.49639 \n",
            "Epoch 9/10 loss 1.69638 val_loss 1.69638 \n",
            "Epoch 9/10 loss 0.80893 val_loss 0.80893 \n",
            "Epoch 9/10 loss 0.68259 val_loss 0.68259 \n",
            "Epoch 9/10 loss 0.50274 val_loss 0.50274 \n",
            "Epoch 9/10 loss 0.36920 val_loss 0.36920 \n",
            "Epoch 9/10 loss -0.35533 val_loss -0.35533 \n",
            "Epoch 9/10 loss -3.41800 val_loss -3.41800 \n",
            "Epoch 9/10 loss -7.73055 val_loss -7.73055 \n",
            "Epoch 9/10 loss -0.32297 val_loss -0.32297 \n",
            "Epoch 9/10 loss -1.76913 val_loss -1.76913 \n",
            "Epoch 9/10 loss 3.31467 val_loss 3.31467 \n",
            "Epoch 9/10 loss 1.00717 val_loss 1.00717 \n",
            "Epoch 9/10 loss 1.03437 val_loss 1.03437 \n",
            "Epoch 9/10 loss 0.15298 val_loss 0.15298 \n",
            "Epoch 9/10 loss 0.14532 val_loss 0.14532 \n",
            "Epoch 9/10 loss 0.14129 val_loss 0.14129 \n",
            "Epoch 9/10 loss -8.92628 val_loss -8.92628 \n",
            "Epoch 9/10 loss 0.12427 val_loss 0.12427 \n",
            "Epoch 9/10 loss 0.11943 val_loss 0.11943 \n",
            "Epoch 9/10 loss 5.59979 val_loss 5.59979 \n",
            "Epoch 9/10 loss 2.26947 val_loss 2.26947 \n",
            "Epoch 9/10 loss 1.18016 val_loss 1.18016 \n",
            "Epoch 9/10 loss 2.21222 val_loss 2.21222 \n",
            "Epoch 9/10 loss 3.14511 val_loss 3.14511 \n",
            "Epoch 10/10 loss -3.55721 val_loss -3.55721 \n",
            "Epoch 10/10 loss -3.64091 val_loss -3.64091 \n",
            "Epoch 10/10 loss 2.50737 val_loss 2.50737 \n",
            "Epoch 10/10 loss -2.40251 val_loss -2.40251 \n",
            "Epoch 10/10 loss -1.28341 val_loss -1.28341 \n",
            "Epoch 10/10 loss -8.81615 val_loss -8.81615 \n",
            "Epoch 10/10 loss 0.34690 val_loss 0.34690 \n",
            "Epoch 10/10 loss -0.08887 val_loss -0.08887 \n",
            "Epoch 10/10 loss 0.33943 val_loss 0.33943 \n",
            "Epoch 10/10 loss -0.62369 val_loss -0.62369 \n",
            "Epoch 10/10 loss 0.29875 val_loss 0.29875 \n",
            "Epoch 10/10 loss -2.65167 val_loss -2.65167 \n",
            "Epoch 10/10 loss -4.95918 val_loss -4.95918 \n",
            "Epoch 10/10 loss -9.69864 val_loss -9.69864 \n",
            "Epoch 10/10 loss -1.08858 val_loss -1.08858 \n",
            "Epoch 10/10 loss 2.39452 val_loss 2.39452 \n",
            "Epoch 10/10 loss 3.22008 val_loss 3.22008 \n",
            "Epoch 10/10 loss -2.72789 val_loss -2.72789 \n",
            "Epoch 10/10 loss -2.11183 val_loss -2.11183 \n",
            "Epoch 10/10 loss 2.69489 val_loss 2.69489 \n",
            "Epoch 10/10 loss -1.58690 val_loss -1.58690 \n",
            "Epoch 10/10 loss 1.07535 val_loss 1.07535 \n",
            "Epoch 10/10 loss -4.52325 val_loss -4.52325 \n",
            "Epoch 10/10 loss 4.17647 val_loss 4.17647 \n",
            "Epoch 10/10 loss 2.13629 val_loss 2.13629 \n",
            "Epoch 10/10 loss 0.13377 val_loss 0.13377 \n",
            "Epoch 10/10 loss -0.80710 val_loss -0.80710 \n",
            "Epoch 10/10 loss -2.93610 val_loss -2.93610 \n",
            "Epoch 10/10 loss -0.77715 val_loss -0.77715 \n",
            "Epoch 10/10 loss 3.72519 val_loss 3.72519 \n",
            "Epoch 10/10 loss -5.44362 val_loss -5.44362 \n",
            "Epoch 10/10 loss -10.22406 val_loss -10.22406 \n",
            "Epoch 10/10 loss -13.19074 val_loss -13.19074 \n",
            "Epoch 10/10 loss -8.85115 val_loss -8.85115 \n",
            "Epoch 10/10 loss 0.05034 val_loss 0.05034 \n",
            "Epoch 10/10 loss 3.85063 val_loss 3.85063 \n",
            "Epoch 10/10 loss 13.31691 val_loss 13.31691 \n",
            "Epoch 10/10 loss -5.12730 val_loss -5.12730 \n",
            "Epoch 10/10 loss 0.00001 val_loss 0.00001 \n",
            "Epoch 10/10 loss 1.45080 val_loss 1.45080 \n",
            "Epoch 10/10 loss -1.76212 val_loss -1.76212 \n",
            "Epoch 10/10 loss 1.47975 val_loss 1.47975 \n",
            "Epoch 10/10 loss -4.81101 val_loss -4.81101 \n",
            "Epoch 10/10 loss -0.03630 val_loss -0.03630 \n",
            "Epoch 10/10 loss 4.02854 val_loss 4.02854 \n",
            "Epoch 10/10 loss 2.53846 val_loss 2.53846 \n",
            "Epoch 10/10 loss 1.22385 val_loss 1.22385 \n",
            "Epoch 10/10 loss -9.38373 val_loss -9.38373 \n",
            "Epoch 10/10 loss -10.32087 val_loss -10.32087 \n",
            "Epoch 10/10 loss -1.53215 val_loss -1.53215 \n",
            "Epoch 10/10 loss 0.99713 val_loss 0.99713 \n",
            "Epoch 10/10 loss 1.74979 val_loss 1.74979 \n",
            "Epoch 10/10 loss -3.02762 val_loss -3.02762 \n",
            "Epoch 10/10 loss 0.24048 val_loss 0.24048 \n",
            "Epoch 10/10 loss 0.26731 val_loss 0.26731 \n",
            "Epoch 10/10 loss 2.46302 val_loss 2.46302 \n",
            "Epoch 10/10 loss 0.33654 val_loss 0.33654 \n",
            "Epoch 10/10 loss 0.37856 val_loss 0.37856 \n",
            "Epoch 10/10 loss 0.08185 val_loss 0.08185 \n",
            "Epoch 10/10 loss 4.93270 val_loss 4.93270 \n",
            "Epoch 10/10 loss 0.45132 val_loss 0.45132 \n",
            "Epoch 10/10 loss 0.99805 val_loss 0.99805 \n",
            "Epoch 10/10 loss 0.72269 val_loss 0.72269 \n",
            "Epoch 10/10 loss 5.70179 val_loss 5.70179 \n",
            "Epoch 10/10 loss 1.03314 val_loss 1.03314 \n",
            "Epoch 10/10 loss 5.02360 val_loss 5.02360 \n",
            "Epoch 10/10 loss 0.64010 val_loss 0.64010 \n",
            "Epoch 10/10 loss 0.70034 val_loss 0.70034 \n",
            "Epoch 10/10 loss 0.69314 val_loss 0.69314 \n",
            "Epoch 10/10 loss 0.67954 val_loss 0.67954 \n",
            "Epoch 10/10 loss -4.45181 val_loss -4.45181 \n",
            "Epoch 10/10 loss 0.73057 val_loss 0.73057 \n",
            "Epoch 10/10 loss 0.60687 val_loss 0.60687 \n",
            "Epoch 10/10 loss 0.52642 val_loss 0.52642 \n",
            "Epoch 10/10 loss 0.03304 val_loss 0.03304 \n",
            "Epoch 10/10 loss -0.45378 val_loss -0.45378 \n",
            "Epoch 10/10 loss 1.52845 val_loss 1.52845 \n",
            "Epoch 10/10 loss 0.32057 val_loss 0.32057 \n",
            "Epoch 10/10 loss -0.90335 val_loss -0.90335 \n",
            "Epoch 10/10 loss 3.03861 val_loss 3.03861 \n",
            "Epoch 10/10 loss 0.19382 val_loss 0.19382 \n",
            "Epoch 10/10 loss 2.66946 val_loss 2.66946 \n",
            "Epoch 10/10 loss -8.20309 val_loss -8.20309 \n",
            "Epoch 10/10 loss -0.75780 val_loss -0.75780 \n",
            "Epoch 10/10 loss -7.70558 val_loss -7.70558 \n",
            "Epoch 10/10 loss -9.43406 val_loss -9.43406 \n",
            "Epoch 10/10 loss -1.07361 val_loss -1.07361 \n",
            "Epoch 10/10 loss 0.93523 val_loss 0.93523 \n",
            "Epoch 10/10 loss -2.49683 val_loss -2.49683 \n",
            "Epoch 10/10 loss -1.41875 val_loss -1.41875 \n",
            "Epoch 10/10 loss 11.78524 val_loss 11.78524 \n",
            "Epoch 10/10 loss -6.38830 val_loss -6.38830 \n",
            "Epoch 10/10 loss -5.70852 val_loss -5.70852 \n",
            "Epoch 10/10 loss 11.68455 val_loss 11.68455 \n",
            "Epoch 10/10 loss 8.80329 val_loss 8.80329 \n",
            "Epoch 10/10 loss 2.52758 val_loss 2.52758 \n",
            "Epoch 10/10 loss -2.43307 val_loss -2.43307 \n",
            "Epoch 10/10 loss 0.00445 val_loss 0.00445 \n",
            "Epoch 10/10 loss -11.10697 val_loss -11.10697 \n",
            "Epoch 10/10 loss 4.52047 val_loss 4.52047 \n",
            "Epoch 10/10 loss -5.65858 val_loss -5.65858 \n",
            "Epoch 10/10 loss -12.18389 val_loss -12.18389 \n",
            "Epoch 10/10 loss -3.47900 val_loss -3.47900 \n",
            "Epoch 10/10 loss 0.00232 val_loss 0.00232 \n",
            "Epoch 10/10 loss 3.08467 val_loss 3.08467 \n",
            "Epoch 10/10 loss 9.51271 val_loss 9.51271 \n",
            "Epoch 10/10 loss 5.74689 val_loss 5.74689 \n",
            "Epoch 10/10 loss 9.45006 val_loss 9.45006 \n",
            "Epoch 10/10 loss -13.38982 val_loss -13.38982 \n",
            "Epoch 10/10 loss 4.99551 val_loss 4.99551 \n",
            "Epoch 10/10 loss -3.83971 val_loss -3.83971 \n",
            "Epoch 10/10 loss 8.96835 val_loss 8.96835 \n",
            "Epoch 10/10 loss 18.82787 val_loss 18.82787 \n",
            "Epoch 10/10 loss 5.86944 val_loss 5.86944 \n",
            "Epoch 10/10 loss 0.00275 val_loss 0.00275 \n",
            "Epoch 10/10 loss -7.36270 val_loss -7.36270 \n",
            "Epoch 10/10 loss -4.74078 val_loss -4.74078 \n",
            "Epoch 10/10 loss 7.00582 val_loss 7.00582 \n",
            "Epoch 10/10 loss 0.15660 val_loss 0.15660 \n",
            "Epoch 10/10 loss -5.66984 val_loss -5.66984 \n",
            "Epoch 10/10 loss 0.90557 val_loss 0.90557 \n",
            "Epoch 10/10 loss 1.56274 val_loss 1.56274 \n",
            "Epoch 10/10 loss 12.43735 val_loss 12.43735 \n",
            "Epoch 10/10 loss -2.19692 val_loss -2.19692 \n",
            "Epoch 10/10 loss -3.44188 val_loss -3.44188 \n",
            "Epoch 10/10 loss 0.03592 val_loss 0.03592 \n",
            "Epoch 10/10 loss -3.28621 val_loss -3.28621 \n",
            "Epoch 10/10 loss 1.81419 val_loss 1.81419 \n",
            "Epoch 10/10 loss -2.68028 val_loss -2.68028 \n",
            "Epoch 10/10 loss -5.27838 val_loss -5.27838 \n",
            "Epoch 10/10 loss -2.13678 val_loss -2.13678 \n",
            "Epoch 10/10 loss -2.73510 val_loss -2.73510 \n",
            "Epoch 10/10 loss 0.25811 val_loss 0.25811 \n",
            "Epoch 10/10 loss 0.05354 val_loss 0.05354 \n",
            "Epoch 10/10 loss -4.79527 val_loss -4.79527 \n",
            "Epoch 10/10 loss 1.59426 val_loss 1.59426 \n",
            "Epoch 10/10 loss 3.20340 val_loss 3.20340 \n",
            "Epoch 10/10 loss 2.90732 val_loss 2.90732 \n",
            "Epoch 10/10 loss 0.90161 val_loss 0.90161 \n",
            "Epoch 10/10 loss 6.15597 val_loss 6.15597 \n",
            "Epoch 10/10 loss 0.04314 val_loss 0.04314 \n",
            "Epoch 10/10 loss -13.38428 val_loss -13.38428 \n",
            "Epoch 10/10 loss 0.03874 val_loss 0.03874 \n",
            "Epoch 10/10 loss -2.26573 val_loss -2.26573 \n",
            "Epoch 10/10 loss 3.61447 val_loss 3.61447 \n",
            "Epoch 10/10 loss 2.88935 val_loss 2.88935 \n",
            "Epoch 10/10 loss -1.70873 val_loss -1.70873 \n",
            "Epoch 10/10 loss 2.84202 val_loss 2.84202 \n",
            "Epoch 10/10 loss -0.60756 val_loss -0.60756 \n",
            "Epoch 10/10 loss -5.55978 val_loss -5.55978 \n",
            "tensor([[-10.5370],\n",
            "        [ -0.9184]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPKvUCYsxCaz"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict(model, X):\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        X = torch.tensor(X).to(device)\n",
        "        pred = model(X)\n",
        "        return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjT3KaN5xFsj",
        "outputId": "647f33ee-a6f8-48b1-ff00-5bbc6dc09b56"
      },
      "source": [
        "sentences = [\"en robótica con fineso\", \"The paper is well\", \"this film is good\", \"a waste of time\"]\n",
        "tokenized = [[tok.text for tok in nlp.tokenizer(sentence)] for sentence in sentences]\n",
        "indexed = [[Texto.vocab.stoi[_t] for _t in t] for t in tokenized]\n",
        "tensor = torch.tensor(indexed).permute(1,0)\n",
        "predictions = torch.argmax(predict(model, tensor), axis=1)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPYmxLulywK9"
      },
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim=128, hidden_dim=128, output_dim=5, num_layers=2, dropout=0.2, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = torch.nn.GRU(\n",
        "            input_size=embedding_dim, \n",
        "            hidden_size=hidden_dim, \n",
        "            num_layers=num_layers, \n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        self.fc = torch.nn.Linear(2*hidden_dim if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        #text = [sent len, batch size]        \n",
        "        embedded = self.embedding(text)        \n",
        "        #embedded = [sent len, batch size, emb dim]        \n",
        "        output, hidden = self.rnn(embedded)        \n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        y = self.fc(output[-1,:,:].squeeze(0))     \n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojyH_MpGy0S0"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def fit(model, dataloader, epochs=5):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss, train_acc = [], []\n",
        "        bar = tqdm(dataloader)\n",
        "        for batch in bar:\n",
        "            X, y = batch.t, batch.v\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "            train_acc.append(acc)\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
        "        bar = tqdm(dataloader)\n",
        "        val_loss, val_acc = [], []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch.text, batch.label\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "                val_acc.append(acc)\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "r2SOuggiykNl",
        "outputId": "6af3ddc2-8b59-43d2-a5d6-cd78df0b6c50"
      },
      "source": [
        "model = RNN(input_dim=len(Texto.vocab), bidirectional=True)\n",
        "fit(model, train_iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/150 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "bar _-->?   0%|          | 0/150 [00:00<?, ?it/s]\n",
            "bacth _--> \n",
            "[torchtext.legacy.data.batch.Batch of size 2]\n",
            "\t[.t]:[torch.cuda.LongTensor of size 446x2 (GPU 0)]\n",
            "\t[.v]:[torch.cuda.LongTensor of size 2 (GPU 0)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-0087c4220ab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTexto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-93-29a1ba70f94d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    }
  ]
}